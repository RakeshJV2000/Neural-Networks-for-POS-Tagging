{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AFN9O00tuIad",
        "pUsa8fUn3ujI",
        "q3Do5MPYw36H",
        "VeRY_5Ts38-A",
        "mmfKd0Fm4CiP"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1.1"
      ],
      "metadata": {
        "id": "AFN9O00tuIad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from collections import defaultdict\n",
        "\n",
        "# Special tokens\n",
        "START_TOKEN = \"<s>\"\n",
        "END_TOKEN = \"</s>\"\n",
        "UNKNOWN_TOKEN = \"UUUNKKK\"\n",
        "\n",
        "def load_data(file_path):\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if sentence:\n",
        "                    sentences.append(sentence)\n",
        "                    sentence = []\n",
        "            else:\n",
        "                word, pos = line.split('\\t')\n",
        "                sentence.append((word, pos))\n",
        "        if sentence:\n",
        "            sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "class POSTaggingDataset(Dataset):\n",
        "    def __init__(self, sentences, word_to_ix, tag_to_ix, context_window):\n",
        "        self.sentences = sentences\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.context_window = context_window\n",
        "\n",
        "    def __len__(self):\n",
        "        return sum(len(sentence) for sentence in self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        for sentence in self.sentences:\n",
        "            if idx < len(sentence):\n",
        "                break\n",
        "            idx -= len(sentence)\n",
        "\n",
        "        context_indices = []\n",
        "        for i in range(-self.context_window, self.context_window + 1):\n",
        "            position = idx + i\n",
        "            if position < 0:\n",
        "                word = START_TOKEN\n",
        "            elif position >= len(sentence):\n",
        "                word = END_TOKEN\n",
        "            else:\n",
        "                word, _ = sentence[position]\n",
        "            context_indices.append(self.word_to_ix.get(word, self.word_to_ix[UNKNOWN_TOKEN]))\n",
        "\n",
        "        _, target_tag = sentence[idx]\n",
        "        target_index = self.tag_to_ix[target_tag]\n",
        "\n",
        "        return torch.tensor(context_indices, dtype=torch.long), torch.tensor(target_index, dtype=torch.long)\n",
        "\n",
        "class FFNNTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim, context_window):\n",
        "        super(FFNNTagger, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.context_window_size = 1 + 2 * context_window\n",
        "\n",
        "        # Initialize embeddings within a specific range\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        nn.init.uniform_(self.embeddings.weight, -0.01, 0.01)\n",
        "\n",
        "        # Initialize hidden layer weights and biases within a specific range\n",
        "        self.hidden_layer = nn.Linear(self.context_window_size * embedding_dim, hidden_dim)\n",
        "        nn.init.uniform_(self.hidden_layer.weight, -0.01, 0.01)\n",
        "        nn.init.uniform_(self.hidden_layer.bias, -0.01, 0.01)\n",
        "\n",
        "        # Initialize output layer weights and biases within a specific range\n",
        "        self.output_layer = nn.Linear(hidden_dim, tagset_size)\n",
        "        nn.init.uniform_(self.output_layer.weight, -0.01, 0.01)\n",
        "        nn.init.uniform_(self.output_layer.bias, -0.01, 0.01)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view(-1)\n",
        "        hidden_out = torch.tanh(self.hidden_layer(embeds))\n",
        "        tag_scores = self.output_layer(hidden_out)\n",
        "        return tag_scores\n",
        "\n",
        "def train_model(train_data_path, dev_data_path, test_data_path, context_window):\n",
        "    # Hyperparameters\n",
        "    EMBEDDING_DIM = 50\n",
        "    HIDDEN_DIM = 128\n",
        "    LEARNING_RATE = 0.02\n",
        "    EPOCHS = 10\n",
        "\n",
        "    # Load data\n",
        "    train_sentences = load_data('twpos-train.tsv')\n",
        "    dev_sentences = load_data('twpos-dev.tsv')\n",
        "    test_sentences = load_data('twpos-devtest.tsv')\n",
        "\n",
        "    # Create vocabulary and tag set mappings\n",
        "    word_to_ix = defaultdict(lambda: len(word_to_ix))\n",
        "    tag_to_ix = defaultdict(lambda: len(tag_to_ix))\n",
        "\n",
        "    # Add special tokens to the vocabulary\n",
        "    word_to_ix[START_TOKEN]\n",
        "    word_to_ix[END_TOKEN]\n",
        "    word_to_ix[UNKNOWN_TOKEN]\n",
        "\n",
        "    # Build vocab and tag index from training data only\n",
        "    for sentence in train_sentences:\n",
        "        for word, tag in sentence:\n",
        "            word_to_ix[word]\n",
        "            tag_to_ix[tag]\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = POSTaggingDataset(train_sentences, word_to_ix, tag_to_ix, context_window)\n",
        "    dev_dataset = POSTaggingDataset(dev_sentences, word_to_ix, tag_to_ix, context_window)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    # Initialize model and optimizer\n",
        "    model = FFNNTagger(len(word_to_ix), len(tag_to_ix), EMBEDDING_DIM, HIDDEN_DIM, context_window)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop with early stopping based on DEV accuracy\n",
        "    best_dev_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for context_indices, target_index in train_loader:\n",
        "            model.zero_grad()\n",
        "\n",
        "            log_probs = model(context_indices)\n",
        "\n",
        "            loss = loss_function(log_probs.view(1,-1), target_index)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "        # Evaluate on DEV set after each epoch to check for early stopping criteria.\n",
        "        dev_accuracy = evaluate_model(model, dev_dataset)\n",
        "\n",
        "        print(f\"DEV Accuracy: {dev_accuracy:.2f}%\")\n",
        "\n",
        "        if dev_accuracy > best_dev_accuracy:\n",
        "            best_dev_accuracy = dev_accuracy\n",
        "\n",
        "    # Final evaluation on DEVTEST set.\n",
        "    test_dataset = POSTaggingDataset(test_sentences, word_to_ix, tag_to_ix, context_window)\n",
        "\n",
        "    test_accuracy = evaluate_model(model, test_dataset)\n",
        "\n",
        "    print(f\"Final DEVTEST Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "def evaluate_model(model, dataset):\n",
        "    model.eval()\n",
        "\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for context_indices, target_index in DataLoader(dataset):\n",
        "            log_probs = model(context_indices)\n",
        "\n",
        "            predicted_index = torch.argmax(log_probs).item()\n",
        "\n",
        "            correct_predictions += (predicted_index == target_index.item())\n",
        "\n",
        "    accuracy_percentage = (correct_predictions / len(dataset)) * 100\n",
        "\n",
        "    return accuracy_percentage\n",
        "\n",
        "print(\"w = 0\")\n",
        "train_model('twpos-train.tsv',\n",
        "            'twpos-dev.tsv',\n",
        "            'twpos-devtest.tsv',\n",
        "            context_window=0)  # For w=0\n",
        "print()\n",
        "print(\"w = 1\")\n",
        "train_model('twpos-train.tsv',\n",
        "            'twpos-dev.tsv',\n",
        "            'twpos-devtest.tsv',\n",
        "            context_window=1)  # For w=1\n"
      ],
      "metadata": {
        "id": "Rs7bglOormfl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad0ffbb3-997f-402a-a1bf-4e0700de57ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = 0\n",
            "Epoch 1/10, Loss: 46328.2555\n",
            "DEV Accuracy: 13.69%\n",
            "Epoch 2/10, Loss: 37205.6970\n",
            "DEV Accuracy: 59.30%\n",
            "Epoch 3/10, Loss: 19245.3192\n",
            "DEV Accuracy: 71.48%\n",
            "Epoch 4/10, Loss: 12726.4430\n",
            "DEV Accuracy: 74.09%\n",
            "Epoch 5/10, Loss: 9848.0026\n",
            "DEV Accuracy: 76.04%\n",
            "Epoch 6/10, Loss: 8445.5727\n",
            "DEV Accuracy: 74.57%\n",
            "Epoch 7/10, Loss: 7412.6244\n",
            "DEV Accuracy: 75.75%\n",
            "Epoch 8/10, Loss: 6555.7232\n",
            "DEV Accuracy: 76.13%\n",
            "Epoch 9/10, Loss: 5929.0838\n",
            "DEV Accuracy: 76.33%\n",
            "Epoch 10/10, Loss: 5537.2412\n",
            "DEV Accuracy: 76.79%\n",
            "Final DEVTEST Accuracy: 77.65%\n",
            "\n",
            "w = 1\n",
            "Epoch 1/10, Loss: 45953.5500\n",
            "DEV Accuracy: 22.86%\n",
            "Epoch 2/10, Loss: 24672.6986\n",
            "DEV Accuracy: 71.06%\n",
            "Epoch 3/10, Loss: 12628.4079\n",
            "DEV Accuracy: 77.99%\n",
            "Epoch 4/10, Loss: 7964.3573\n",
            "DEV Accuracy: 79.82%\n",
            "Epoch 5/10, Loss: 5970.2712\n",
            "DEV Accuracy: 79.92%\n",
            "Epoch 6/10, Loss: 4998.0739\n",
            "DEV Accuracy: 80.15%\n",
            "Epoch 7/10, Loss: 3940.5087\n",
            "DEV Accuracy: 79.13%\n",
            "Epoch 8/10, Loss: 3083.0431\n",
            "DEV Accuracy: 80.61%\n",
            "Epoch 9/10, Loss: 2514.0898\n",
            "DEV Accuracy: 80.19%\n",
            "Epoch 10/10, Loss: 1990.3404\n",
            "DEV Accuracy: 79.88%\n",
            "Final DEVTEST Accuracy: 81.44%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 Feature Engineering"
      ],
      "metadata": {
        "id": "pUsa8fUn3ujI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class POSTaggingDataset2(Dataset):\n",
        "    def __init__(self, sentences, word_to_ix, tag_to_ix, context_window):\n",
        "        self.sentences = sentences\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.context_window = context_window\n",
        "\n",
        "    def __len__(self):\n",
        "        return sum(len(sentence) for sentence in self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        for sentence in self.sentences:\n",
        "            if idx < len(sentence):\n",
        "                break\n",
        "            idx -= len(sentence)\n",
        "\n",
        "        context_indices = []\n",
        "        for i in range(-self.context_window, self.context_window + 1):\n",
        "            position = idx + i\n",
        "            if position < 0:\n",
        "                word = START_TOKEN\n",
        "            elif position >= len(sentence):\n",
        "                word = END_TOKEN\n",
        "            else:\n",
        "                word, _ = sentence[position]\n",
        "            context_indices.append(self.word_to_ix.get(word, self.word_to_ix[UNKNOWN_TOKEN]))\n",
        "\n",
        "        # Extract additional features for the center word\n",
        "        center_word, target_tag = sentence[idx]\n",
        "        target_index = self.tag_to_ix[target_tag]\n",
        "\n",
        "        # Feature 1: Capitalization (binary)\n",
        "        is_capitalized = 1 if center_word[0].isupper() else 0\n",
        "\n",
        "        # Feature 2: Suffixes (binary for common suffixes)\n",
        "        has_suffix_ing = 1 if center_word.endswith(\"ing\") else 0\n",
        "        has_suffix_ed = 1 if center_word.endswith(\"ed\") else 0\n",
        "        has_suffix_ly = 1 if center_word.endswith(\"ly\") else 0\n",
        "\n",
        "        # Feature 3: Special characters (binary)\n",
        "        contains_special_char = 1 if any(char in \"!@#?&\" for char in center_word) else 0\n",
        "\n",
        "\n",
        "        features = [is_capitalized, has_suffix_ing, has_suffix_ed, has_suffix_ly, contains_special_char]\n",
        "        # print(f\"Features for word '{center_word}': {features}\")  # Add this print statement\n",
        "\n",
        "        features_tensor = torch.tensor(features, dtype=torch.float32)\n",
        "\n",
        "        return torch.tensor(context_indices, dtype=torch.long), features_tensor, torch.tensor(target_index, dtype=torch.long)\n",
        "\n",
        "\n",
        "class FFNNTagger2(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim, context_window, feature_dim):\n",
        "        super(FFNNTagger2, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.context_window_size = 1 + 2 * context_window\n",
        "        self.feature_dim = feature_dim\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        nn.init.uniform_(self.embeddings.weight, -0.01, 0.01)\n",
        "\n",
        "        input_size = self.context_window_size * embedding_dim + feature_dim\n",
        "        self.hidden_layer = nn.Linear(input_size, hidden_dim)\n",
        "        nn.init.uniform_(self.hidden_layer.weight, -0.01, 0.01)\n",
        "        nn.init.uniform_(self.hidden_layer.bias, -0.01, 0.01)\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(hidden_dim, tagset_size)\n",
        "        nn.init.uniform_(self.output_layer.weight, -0.01, 0.01)\n",
        "        nn.init.uniform_(self.output_layer.bias, -0.01, 0.01)\n",
        "\n",
        "    def forward(self, inputs, features):\n",
        "\n",
        "        embeds = self.embeddings(inputs).view(-1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            features = features.squeeze(0)\n",
        "\n",
        "        # Concatenate the features with the embeddings\n",
        "        combined_input = torch.cat((embeds, features), dim=0)\n",
        "\n",
        "        # Pass through the hidden layer\n",
        "        hidden_out = torch.tanh(self.hidden_layer(combined_input))\n",
        "\n",
        "        # Output layer\n",
        "        tag_scores = self.output_layer(hidden_out)\n",
        "\n",
        "        return tag_scores\n",
        "\n",
        "\n",
        "def train_model_features(train_data_path, dev_data_path, test_data_path, context_window):\n",
        "    # Hyperparameters\n",
        "    EMBEDDING_DIM = 50\n",
        "    HIDDEN_DIM = 128\n",
        "    LEARNING_RATE = 0.02\n",
        "    EPOCHS = 10\n",
        "    FEATURE_DIM = 5\n",
        "\n",
        "    # Load data\n",
        "    train_sentences = load_data(train_data_path)\n",
        "    dev_sentences = load_data(dev_data_path)\n",
        "    test_sentences = load_data(test_data_path)\n",
        "\n",
        "    # Create vocabulary and tag set mappings\n",
        "    word_to_ix = defaultdict(lambda: len(word_to_ix))\n",
        "    tag_to_ix = defaultdict(lambda: len(tag_to_ix))\n",
        "\n",
        "    # Add special tokens to the vocabulary\n",
        "    word_to_ix[START_TOKEN]\n",
        "    word_to_ix[END_TOKEN]\n",
        "    word_to_ix[UNKNOWN_TOKEN]\n",
        "\n",
        "    # Build vocab and tag index from training data only\n",
        "    for sentence in train_sentences:\n",
        "        for word, tag in sentence:\n",
        "            word_to_ix[word]\n",
        "            tag_to_ix[tag]\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = POSTaggingDataset2(train_sentences, word_to_ix, tag_to_ix, context_window)\n",
        "    dev_dataset = POSTaggingDataset2(dev_sentences, word_to_ix, tag_to_ix, context_window)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    # Initialize model and optimizer\n",
        "    model = FFNNTagger2(len(word_to_ix), len(tag_to_ix), EMBEDDING_DIM, HIDDEN_DIM, context_window, FEATURE_DIM)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop with early stopping based on DEV accuracy\n",
        "    best_dev_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for batch_idx, (context_indices, features, target_index) in enumerate(train_loader):\n",
        "            # print(f\"Processing batch {batch_idx}\")  # Print to verify batches are being processed\n",
        "            # print(f\"Context indices: {context_indices}\")  # Print the context indices\n",
        "            # print(f\"Features: {features}\")  # Print the features extracted\n",
        "            # print(f\"Target index: {target_index}\")  # Print the target index\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            log_probs = model(context_indices, features)\n",
        "\n",
        "            loss = loss_function(log_probs.view(1, -1), target_index)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "        # Evaluate on DEV set after each epoch\n",
        "        dev_accuracy = evaluate_model_features(model, dev_dataset)\n",
        "        print(f\"DEV Accuracy: {dev_accuracy:.2f}%\")\n",
        "\n",
        "        if dev_accuracy > best_dev_accuracy:\n",
        "            best_dev_accuracy = dev_accuracy\n",
        "\n",
        "    # Final evaluation on DEVTEST set\n",
        "    test_dataset = POSTaggingDataset2(test_sentences, word_to_ix, tag_to_ix, context_window)\n",
        "    test_accuracy = evaluate_model_features(model, test_dataset)\n",
        "    print(f\"Final DEVTEST Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "def evaluate_model_features(model, dataset):\n",
        "    model.eval()\n",
        "\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for context_indices, features, target_index in DataLoader(dataset):\n",
        "            log_probs = model(context_indices, features)\n",
        "\n",
        "            predicted_index = torch.argmax(log_probs).item()\n",
        "            correct_predictions += (predicted_index == target_index.item())\n",
        "\n",
        "    accuracy_percentage = (correct_predictions / len(dataset)) * 100\n",
        "    return accuracy_percentage\n",
        "\n",
        "print(\"w = 0\")\n",
        "train_model_features('twpos-train.tsv',\n",
        "            'twpos-dev.tsv',\n",
        "            'twpos-devtest.tsv',\n",
        "            context_window=0)\n",
        "print()\n",
        "print(\"w = 1\")\n",
        "train_model_features('twpos-train.tsv',\n",
        "            'twpos-dev.tsv',\n",
        "            'twpos-devtest.tsv',\n",
        "            context_window=1)"
      ],
      "metadata": {
        "id": "bI3jxOvXOnGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f0dad59-eeff-48c3-8aaf-1f7e7584e03e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = 0\n",
            "Epoch 1/10, Loss: 36689.1398\n",
            "DEV Accuracy: 49.57%\n",
            "Epoch 2/10, Loss: 20098.2078\n",
            "DEV Accuracy: 69.07%\n",
            "Epoch 3/10, Loss: 13695.4710\n",
            "DEV Accuracy: 75.46%\n",
            "Epoch 4/10, Loss: 10728.6314\n",
            "DEV Accuracy: 77.93%\n",
            "Epoch 5/10, Loss: 8200.0050\n",
            "DEV Accuracy: 79.34%\n",
            "Epoch 6/10, Loss: 6750.3968\n",
            "DEV Accuracy: 79.07%\n",
            "Epoch 7/10, Loss: 5968.8906\n",
            "DEV Accuracy: 79.65%\n",
            "Epoch 8/10, Loss: 5466.1237\n",
            "DEV Accuracy: 79.38%\n",
            "Epoch 9/10, Loss: 5151.2382\n",
            "DEV Accuracy: 77.04%\n",
            "Epoch 10/10, Loss: 4894.4456\n",
            "DEV Accuracy: 79.26%\n",
            "Final DEVTEST Accuracy: 81.05%\n",
            "\n",
            "w = 1\n",
            "Epoch 1/10, Loss: 35523.9398\n",
            "DEV Accuracy: 58.78%\n",
            "Epoch 2/10, Loss: 15516.8664\n",
            "DEV Accuracy: 77.66%\n",
            "Epoch 3/10, Loss: 8552.6263\n",
            "DEV Accuracy: 81.00%\n",
            "Epoch 4/10, Loss: 5794.2820\n",
            "DEV Accuracy: 81.44%\n",
            "Epoch 5/10, Loss: 4566.8702\n",
            "DEV Accuracy: 81.60%\n",
            "Epoch 6/10, Loss: 3631.7004\n",
            "DEV Accuracy: 82.00%\n",
            "Epoch 7/10, Loss: 2967.7965\n",
            "DEV Accuracy: 81.75%\n",
            "Epoch 8/10, Loss: 2314.1690\n",
            "DEV Accuracy: 81.77%\n",
            "Epoch 9/10, Loss: 1878.1420\n",
            "DEV Accuracy: 81.25%\n",
            "Epoch 10/10, Loss: 1445.3458\n",
            "DEV Accuracy: 81.54%\n",
            "Final DEVTEST Accuracy: 82.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 .1 Training the pre trained embeddings"
      ],
      "metadata": {
        "id": "q3Do5MPYw36H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n",
        "\n",
        "# Special tokens\n",
        "START_TOKEN = \"<s>\"\n",
        "END_TOKEN = \"</s>\"\n",
        "UNKNOWN_TOKEN = \"UUUNKKK\"\n",
        "\n",
        "# Load data from file\n",
        "def load_data(file_path):\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if sentence:\n",
        "                    sentences.append(sentence)\n",
        "                    sentence = []\n",
        "            else:\n",
        "                word, pos = line.split('\\t')\n",
        "                sentence.append((word, pos))\n",
        "        if sentence:\n",
        "            sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "# Load pretrained embeddings from twitter-embeddings.txt\n",
        "def load_pretrained_embeddings(filepath, embedding_dim):\n",
        "    embeddings = {}\n",
        "    with open(filepath, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            word = parts[0]\n",
        "            vector = np.array(parts[1:], dtype=np.float32)\n",
        "            embeddings[word] = vector\n",
        "\n",
        "    # Initialize special tokens\n",
        "    embeddings[START_TOKEN] = embeddings.get(END_TOKEN)\n",
        "    embeddings[UNKNOWN_TOKEN] = embeddings.get(UNKNOWN_TOKEN, np.random.uniform(-0.01, 0.01, embedding_dim))\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "class POSTaggingDataset3(Dataset):\n",
        "    def __init__(self, sentences, word_to_ix, tag_to_ix, pretrained_embeddings, embedding_dim, context_window):\n",
        "        self.sentences = sentences\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.context_window = context_window\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.pretrained_embeddings = pretrained_embeddings\n",
        "\n",
        "    def __len__(self):\n",
        "        return sum(len(sentence) for sentence in self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        for sentence in self.sentences:\n",
        "            if idx < len(sentence):\n",
        "                break\n",
        "            idx -= len(sentence)\n",
        "\n",
        "        context_indices = []\n",
        "        for i in range(-self.context_window, self.context_window + 1):\n",
        "            position = idx + i\n",
        "            if position < 0:\n",
        "                word = START_TOKEN\n",
        "            elif position >= len(sentence):\n",
        "                word = END_TOKEN\n",
        "            else:\n",
        "                word, _ = sentence[position]\n",
        "            context_indices.append(self.word_to_ix.get(word, self.word_to_ix[UNKNOWN_TOKEN]))\n",
        "\n",
        "        # Extract word and POS tag for the center word\n",
        "        center_word, target_tag = sentence[idx]\n",
        "        target_index = self.tag_to_ix[target_tag]\n",
        "\n",
        "        return torch.tensor(context_indices, dtype=torch.long), torch.tensor(target_index, dtype=torch.long)\n",
        "\n",
        "class FFNNTagger3(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim, context_window, pretrained_embeddings, word_to_ix):\n",
        "        super(FFNNTagger3, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.context_window_size = 1 + 2 * context_window\n",
        "\n",
        "        # Initialize an embedding layer using pretrained embeddings\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Initialize the embedding weights with the pretrained embeddings\n",
        "        pretrained_weight = self.initialize_pretrained_weights(vocab_size, embedding_dim, pretrained_embeddings, word_to_ix)\n",
        "        self.embeddings.weight.data.copy_(torch.tensor(pretrained_weight))\n",
        "\n",
        "        # Hidden layer\n",
        "        self.hidden_layer = nn.Linear(self.context_window_size * embedding_dim, hidden_dim)\n",
        "        nn.init.uniform_(self.hidden_layer.weight, -0.01, 0.01)\n",
        "        nn.init.uniform_(self.hidden_layer.bias, -0.01, 0.01)\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(hidden_dim, tagset_size)\n",
        "        nn.init.uniform_(self.output_layer.weight, -0.01, 0.01)\n",
        "        nn.init.uniform_(self.output_layer.bias, -0.01, 0.01)\n",
        "\n",
        "    def initialize_pretrained_weights(self, vocab_size, embedding_dim, pretrained_embeddings, word_to_ix):\n",
        "        pretrained_weight = np.random.uniform(-0.01, 0.01, (vocab_size, embedding_dim))\n",
        "        for word, idx in word_to_ix.items():\n",
        "            if word in pretrained_embeddings:\n",
        "                pretrained_weight[idx] = pretrained_embeddings[word]\n",
        "        return pretrained_weight\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Get the embeddings for the input word indices\n",
        "        embeds = self.embeddings(inputs).view(-1)\n",
        "\n",
        "        # Pass through the hidden layer\n",
        "        hidden_out = torch.tanh(self.hidden_layer(embeds))\n",
        "\n",
        "        # Output layer\n",
        "        tag_scores = self.output_layer(hidden_out)\n",
        "        return tag_scores\n",
        "\n",
        "\n",
        "def train_model_pretrained(train_data_path, dev_data_path, test_data_path, embedding_path, context_window):\n",
        "    # Hyperparameters\n",
        "    EMBEDDING_DIM = 50\n",
        "    HIDDEN_DIM = 128\n",
        "    LEARNING_RATE = 0.02\n",
        "    EPOCHS = 10\n",
        "\n",
        "    # Load data\n",
        "    train_sentences = load_data(train_data_path)\n",
        "    dev_sentences = load_data(dev_data_path)\n",
        "    test_sentences = load_data(test_data_path)\n",
        "\n",
        "    # Load pretrained embeddings\n",
        "    pretrained_embeddings = load_pretrained_embeddings(embedding_path, EMBEDDING_DIM)\n",
        "\n",
        "    # Create vocabulary and tag set mappings\n",
        "    word_to_ix = defaultdict(lambda: len(word_to_ix))\n",
        "    tag_to_ix = defaultdict(lambda: len(tag_to_ix))\n",
        "\n",
        "    # Add special tokens to the vocabulary\n",
        "    word_to_ix[START_TOKEN]\n",
        "    word_to_ix[END_TOKEN]\n",
        "    word_to_ix[UNKNOWN_TOKEN]\n",
        "\n",
        "    # Build vocab and tag index from training data only\n",
        "    for sentence in train_sentences:\n",
        "        for word, tag in sentence:\n",
        "            word_to_ix[word]\n",
        "            tag_to_ix[tag]\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = POSTaggingDataset3(train_sentences, word_to_ix, tag_to_ix, pretrained_embeddings, EMBEDDING_DIM, context_window)\n",
        "    dev_dataset = POSTaggingDataset3(dev_sentences, word_to_ix, tag_to_ix, pretrained_embeddings, EMBEDDING_DIM, context_window)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    # Initialize model and optimizer\n",
        "    model = FFNNTagger3(len(word_to_ix), len(tag_to_ix), EMBEDDING_DIM, HIDDEN_DIM, context_window, pretrained_embeddings, word_to_ix)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    best_dev_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for context_indices, target_index in train_loader:\n",
        "            model.zero_grad()\n",
        "\n",
        "            log_probs = model(context_indices)\n",
        "\n",
        "            loss = loss_function(log_probs.view(1, -1), target_index)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "        # Evaluate on DEV set after each epoch\n",
        "        dev_accuracy = evaluate_model_pretrained(model, dev_dataset)\n",
        "        print(f\"DEV Accuracy: {dev_accuracy:.2f}%\")\n",
        "\n",
        "        if dev_accuracy > best_dev_accuracy:\n",
        "            best_dev_accuracy = dev_accuracy\n",
        "\n",
        "    # Final evaluation on DEVTEST set\n",
        "    test_dataset = POSTaggingDataset3(test_sentences, word_to_ix, tag_to_ix, pretrained_embeddings, EMBEDDING_DIM, context_window)\n",
        "    test_accuracy = evaluate_model_pretrained(model, test_dataset)\n",
        "    print(f\"Final DEVTEST Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "def evaluate_model_pretrained(model, dataset):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for context_indices, target_index in DataLoader(dataset):\n",
        "            log_probs = model(context_indices)\n",
        "\n",
        "            predicted_index = torch.argmax(log_probs).item()\n",
        "            correct_predictions += (predicted_index == target_index.item())\n",
        "\n",
        "    accuracy_percentage = (correct_predictions / len(dataset)) * 100\n",
        "    return accuracy_percentage\n",
        "\n",
        "print(\"w = 0\")\n",
        "train_model_pretrained('twpos-train.tsv',\n",
        "                       'twpos-dev.tsv',\n",
        "                       'twpos-devtest.tsv',\n",
        "                       'twitter-embeddings.txt',\n",
        "                       context_window=0)\n",
        "print()\n",
        "print(\"w = 1\")\n",
        "train_model_pretrained('twpos-train.tsv',\n",
        "                       'twpos-dev.tsv',\n",
        "                       'twpos-devtest.tsv',\n",
        "                       'twitter-embeddings.txt',\n",
        "                       context_window=1)"
      ],
      "metadata": {
        "id": "4gEnz6F1qwGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d625de0b-3fe2-400b-8df0-414590ce396d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = 0\n",
            "Epoch 1/10, Loss: 17579.4120\n",
            "DEV Accuracy: 76.77%\n",
            "Epoch 2/10, Loss: 7775.4920\n",
            "DEV Accuracy: 77.39%\n",
            "Epoch 3/10, Loss: 5768.5907\n",
            "DEV Accuracy: 76.37%\n",
            "Epoch 4/10, Loss: 4904.8336\n",
            "DEV Accuracy: 76.10%\n",
            "Epoch 5/10, Loss: 4576.7104\n",
            "DEV Accuracy: 76.87%\n",
            "Epoch 6/10, Loss: 4263.7991\n",
            "DEV Accuracy: 76.71%\n",
            "Epoch 7/10, Loss: 4081.6675\n",
            "DEV Accuracy: 76.93%\n",
            "Epoch 8/10, Loss: 3896.2557\n",
            "DEV Accuracy: 76.91%\n",
            "Epoch 9/10, Loss: 3836.2148\n",
            "DEV Accuracy: 76.95%\n",
            "Epoch 10/10, Loss: 3703.6257\n",
            "DEV Accuracy: 76.81%\n",
            "Final DEVTEST Accuracy: 77.39%\n",
            "\n",
            "w = 1\n",
            "Epoch 1/10, Loss: 15623.8788\n",
            "DEV Accuracy: 79.03%\n",
            "Epoch 2/10, Loss: 5944.0927\n",
            "DEV Accuracy: 81.04%\n",
            "Epoch 3/10, Loss: 3575.9568\n",
            "DEV Accuracy: 80.75%\n",
            "Epoch 4/10, Loss: 2399.1395\n",
            "DEV Accuracy: 80.71%\n",
            "Epoch 5/10, Loss: 1686.6299\n",
            "DEV Accuracy: 82.43%\n",
            "Epoch 6/10, Loss: 1266.8942\n",
            "DEV Accuracy: 81.37%\n",
            "Epoch 7/10, Loss: 962.5539\n",
            "DEV Accuracy: 82.02%\n",
            "Epoch 8/10, Loss: 760.1283\n",
            "DEV Accuracy: 82.93%\n",
            "Epoch 9/10, Loss: 567.7690\n",
            "DEV Accuracy: 82.04%\n",
            "Epoch 10/10, Loss: 455.3511\n",
            "DEV Accuracy: 81.85%\n",
            "Final DEVTEST Accuracy: 82.58%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 .2 compare"
      ],
      "metadata": {
        "id": "VeRY_5Ts38-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class POSTaggingDataset4(Dataset):\n",
        "    def __init__(self, sentences, word_to_ix, tag_to_ix, pretrained_embeddings, embedding_dim, context_window):\n",
        "        self.sentences = sentences\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.context_window = context_window\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.pretrained_embeddings = pretrained_embeddings\n",
        "\n",
        "    def __len__(self):\n",
        "        return sum(len(sentence) for sentence in self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        for sentence in self.sentences:\n",
        "            if idx < len(sentence):\n",
        "                break\n",
        "            idx -= len(sentence)\n",
        "\n",
        "        context_indices = []\n",
        "        for i in range(-self.context_window, self.context_window + 1):\n",
        "            position = idx + i\n",
        "            if position < 0:\n",
        "                word = START_TOKEN\n",
        "            elif position >= len(sentence):\n",
        "                word = END_TOKEN\n",
        "            else:\n",
        "                word, _ = sentence[position]\n",
        "            context_indices.append(self.word_to_ix.get(word, self.word_to_ix[UNKNOWN_TOKEN]))\n",
        "\n",
        "        # Extract word and POS tag for the center word\n",
        "        center_word, target_tag = sentence[idx]\n",
        "        target_index = self.tag_to_ix[target_tag]\n",
        "\n",
        "        return torch.tensor(context_indices, dtype=torch.long), torch.tensor(target_index, dtype=torch.long)\n",
        "\n",
        "class FFNNTagger4(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim, context_window, pretrained_embeddings, word_to_ix):\n",
        "        super(FFNNTagger4, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.context_window_size = 1 + 2 * context_window\n",
        "\n",
        "        # Initialize an embedding layer using pretrained embeddings\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Initialize the embedding weights with the pretrained embeddings\n",
        "        pretrained_weight = self.initialize_pretrained_weights(vocab_size, embedding_dim, pretrained_embeddings, word_to_ix)\n",
        "        self.embeddings.weight.data.copy_(torch.tensor(pretrained_weight))\n",
        "\n",
        "        # Set embedding layer to not update (freeze embeddings)\n",
        "        self.embeddings.weight.requires_grad = False\n",
        "\n",
        "        # Hidden layer\n",
        "        self.hidden_layer = nn.Linear(self.context_window_size * embedding_dim, hidden_dim)\n",
        "        nn.init.uniform_(self.hidden_layer.weight, -0.01, 0.01)\n",
        "        nn.init.uniform_(self.hidden_layer.bias, -0.01, 0.01)\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(hidden_dim, tagset_size)\n",
        "        nn.init.uniform_(self.output_layer.weight, -0.01, 0.01)\n",
        "        nn.init.uniform_(self.output_layer.bias, -0.01, 0.01)\n",
        "\n",
        "    def initialize_pretrained_weights(self, vocab_size, embedding_dim, pretrained_embeddings, word_to_ix):\n",
        "        pretrained_weight = np.random.uniform(-0.01, 0.01, (vocab_size, embedding_dim))\n",
        "        for word, idx in word_to_ix.items():\n",
        "            if word in pretrained_embeddings:\n",
        "                pretrained_weight[idx] = pretrained_embeddings[word]\n",
        "        return pretrained_weight\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Get the embeddings for the input word indices\n",
        "        embeds = self.embeddings(inputs).view(-1)\n",
        "\n",
        "        # Pass through the hidden layer\n",
        "        hidden_out = torch.tanh(self.hidden_layer(embeds))\n",
        "\n",
        "        # Output layer\n",
        "        tag_scores = self.output_layer(hidden_out)\n",
        "        return tag_scores\n",
        "\n",
        "def train_model_static_embeddings(train_data_path, dev_data_path, test_data_path, embedding_path, context_window):\n",
        "    # Hyperparameters\n",
        "    EMBEDDING_DIM = 50\n",
        "    HIDDEN_DIM = 128\n",
        "    LEARNING_RATE = 0.02\n",
        "    EPOCHS = 10\n",
        "\n",
        "    # Load data\n",
        "    train_sentences = load_data(train_data_path)\n",
        "    dev_sentences = load_data(dev_data_path)\n",
        "    test_sentences = load_data(test_data_path)\n",
        "\n",
        "    # Load pretrained embeddings\n",
        "    pretrained_embeddings = load_pretrained_embeddings(embedding_path, EMBEDDING_DIM)\n",
        "\n",
        "    # Create vocabulary and tag set mappings\n",
        "    word_to_ix = defaultdict(lambda: len(word_to_ix))\n",
        "    tag_to_ix = defaultdict(lambda: len(tag_to_ix))\n",
        "\n",
        "    # Add special tokens to the vocabulary\n",
        "    word_to_ix[START_TOKEN]\n",
        "    word_to_ix[END_TOKEN]\n",
        "    word_to_ix[UNKNOWN_TOKEN]\n",
        "\n",
        "    # Build vocab and tag index from training data only\n",
        "    for sentence in train_sentences:\n",
        "        for word, tag in sentence:\n",
        "            word_to_ix[word]\n",
        "            tag_to_ix[tag]\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = POSTaggingDataset4(train_sentences, word_to_ix, tag_to_ix, pretrained_embeddings, EMBEDDING_DIM, context_window)\n",
        "    dev_dataset = POSTaggingDataset4(dev_sentences, word_to_ix, tag_to_ix, pretrained_embeddings, EMBEDDING_DIM, context_window)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    # Initialize model and optimizer\n",
        "    model = FFNNTagger4(len(word_to_ix), len(tag_to_ix), EMBEDDING_DIM, HIDDEN_DIM, context_window, pretrained_embeddings, word_to_ix)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    best_dev_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for context_indices, target_index in train_loader:\n",
        "            model.zero_grad()\n",
        "\n",
        "            log_probs = model(context_indices)\n",
        "\n",
        "            loss = loss_function(log_probs.view(1, -1), target_index)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "        # Evaluate on DEV set after each epoch\n",
        "        dev_accuracy = evaluate_model_static_embeddings(model, dev_dataset)\n",
        "        print(f\"DEV Accuracy: {dev_accuracy:.2f}%\")\n",
        "\n",
        "        if dev_accuracy > best_dev_accuracy:\n",
        "            best_dev_accuracy = dev_accuracy\n",
        "\n",
        "    # Final evaluation on DEVTEST set\n",
        "    test_dataset = POSTaggingDataset4(test_sentences, word_to_ix, tag_to_ix, pretrained_embeddings, EMBEDDING_DIM, context_window)\n",
        "    test_accuracy = evaluate_model_static_embeddings(model, test_dataset)\n",
        "    print(f\"Final DEVTEST Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "def evaluate_model_static_embeddings(model, dataset):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for context_indices, target_index in DataLoader(dataset):\n",
        "            log_probs = model(context_indices)\n",
        "\n",
        "            predicted_index = torch.argmax(log_probs).item()\n",
        "            correct_predictions += (predicted_index == target_index.item())\n",
        "\n",
        "    accuracy_percentage = (correct_predictions / len(dataset)) * 100\n",
        "    return accuracy_percentage\n",
        "\n",
        "print(\"w = 0\")\n",
        "train_model_static_embeddings('twpos-train.tsv',\n",
        "                              'twpos-dev.tsv',\n",
        "                              'twpos-devtest.tsv',\n",
        "                              'twitter-embeddings.txt',\n",
        "                              context_window=0)\n",
        "print()\n",
        "print(\"w = 1\")\n",
        "train_model_static_embeddings('twpos-train.tsv',\n",
        "                              'twpos-dev.tsv',\n",
        "                              'twpos-devtest.tsv',\n",
        "                              'twitter-embeddings.txt',\n",
        "                              context_window=1)"
      ],
      "metadata": {
        "id": "YFg_l8Ukt2mI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c9f06b3-8e37-47c3-fc93-e67367cb96c7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = 0\n",
            "Epoch 1/10, Loss: 19088.1056\n",
            "DEV Accuracy: 73.12%\n",
            "Epoch 2/10, Loss: 10974.5385\n",
            "DEV Accuracy: 74.94%\n",
            "Epoch 3/10, Loss: 10303.2349\n",
            "DEV Accuracy: 76.13%\n",
            "Epoch 4/10, Loss: 9959.6109\n",
            "DEV Accuracy: 76.21%\n",
            "Epoch 5/10, Loss: 9741.4381\n",
            "DEV Accuracy: 76.29%\n",
            "Epoch 6/10, Loss: 9489.0405\n",
            "DEV Accuracy: 76.89%\n",
            "Epoch 7/10, Loss: 9329.2530\n",
            "DEV Accuracy: 76.79%\n",
            "Epoch 8/10, Loss: 9127.5487\n",
            "DEV Accuracy: 76.64%\n",
            "Epoch 9/10, Loss: 8931.1695\n",
            "DEV Accuracy: 77.35%\n",
            "Epoch 10/10, Loss: 8753.0892\n",
            "DEV Accuracy: 77.43%\n",
            "Final DEVTEST Accuracy: 77.60%\n",
            "\n",
            "w = 1\n",
            "Epoch 1/10, Loss: 17296.4064\n",
            "DEV Accuracy: 76.42%\n",
            "Epoch 2/10, Loss: 9349.2565\n",
            "DEV Accuracy: 77.99%\n",
            "Epoch 3/10, Loss: 8416.2451\n",
            "DEV Accuracy: 78.64%\n",
            "Epoch 4/10, Loss: 7892.2359\n",
            "DEV Accuracy: 78.10%\n",
            "Epoch 5/10, Loss: 7484.8794\n",
            "DEV Accuracy: 79.78%\n",
            "Epoch 6/10, Loss: 7164.8380\n",
            "DEV Accuracy: 80.00%\n",
            "Epoch 7/10, Loss: 6860.9075\n",
            "DEV Accuracy: 80.00%\n",
            "Epoch 8/10, Loss: 6593.1387\n",
            "DEV Accuracy: 79.86%\n",
            "Epoch 9/10, Loss: 6282.2943\n",
            "DEV Accuracy: 79.61%\n",
            "Epoch 10/10, Loss: 6024.6203\n",
            "DEV Accuracy: 81.14%\n",
            "Final DEVTEST Accuracy: 81.03%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 .3 embeddings with features"
      ],
      "metadata": {
        "id": "mmfKd0Fm4CiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class POSTaggingDataset5(Dataset):\n",
        "    def __init__(self, sentences, word_to_ix, tag_to_ix, pretrained_embeddings, embedding_dim, context_window):\n",
        "        self.sentences = sentences\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.context_window = context_window\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.pretrained_embeddings = pretrained_embeddings\n",
        "\n",
        "    def __len__(self):\n",
        "        return sum(len(sentence) for sentence in self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        for sentence in self.sentences:\n",
        "            if idx < len(sentence):\n",
        "                break\n",
        "            idx -= len(sentence)\n",
        "\n",
        "        context_indices = []\n",
        "        for i in range(-self.context_window, self.context_window + 1):\n",
        "            position = idx + i\n",
        "            if position < 0:\n",
        "                word = START_TOKEN\n",
        "            elif position >= len(sentence):\n",
        "                word = END_TOKEN\n",
        "            else:\n",
        "                word, _ = sentence[position]\n",
        "            context_indices.append(self.word_to_ix.get(word, self.word_to_ix[UNKNOWN_TOKEN]))\n",
        "\n",
        "        # Extract word and POS tag for the center word\n",
        "        center_word, target_tag = sentence[idx]\n",
        "        target_index = self.tag_to_ix[target_tag]\n",
        "\n",
        "        # Feature 1: Capitalization (binary)\n",
        "        is_capitalized = 1 if center_word[0].isupper() else 0\n",
        "\n",
        "        # Feature 2: Suffixes (binary for common suffixes)\n",
        "        has_suffix_ing = 1 if center_word.endswith(\"ing\") else 0\n",
        "        has_suffix_ed = 1 if center_word.endswith(\"ed\") else 0\n",
        "        has_suffix_ly = 1 if center_word.endswith(\"ly\") else 0\n",
        "\n",
        "        # Feature 3: Special characters (binary)\n",
        "        contains_special_char = 1 if any(char in \"!@#?&\" for char in center_word) else 0\n",
        "\n",
        "        # Combine features into a tensor\n",
        "        features = [is_capitalized, has_suffix_ing, has_suffix_ed, has_suffix_ly, contains_special_char]\n",
        "        features_tensor = torch.tensor(features, dtype=torch.float32)\n",
        "\n",
        "        return torch.tensor(context_indices, dtype=torch.long), features_tensor, torch.tensor(target_index, dtype=torch.long)\n",
        "\n",
        "class FFNNTagger5(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim, context_window, feature_dim, pretrained_embeddings, word_to_ix):\n",
        "        super(FFNNTagger5, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.context_window_size = 1 + 2 * context_window\n",
        "        self.feature_dim = feature_dim\n",
        "\n",
        "        # Initialize an embedding layer using pretrained embeddings\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Initialize the embedding weights with the pretrained embeddings\n",
        "        pretrained_weight = self.initialize_pretrained_weights(vocab_size, embedding_dim, pretrained_embeddings, word_to_ix)\n",
        "        self.embeddings.weight.data.copy_(torch.tensor(pretrained_weight))\n",
        "\n",
        "        # Hidden layer: Adjust input to also include the extra feature dimensions\n",
        "        input_size = self.context_window_size * embedding_dim + feature_dim\n",
        "        self.hidden_layer = nn.Linear(input_size, hidden_dim)\n",
        "        nn.init.uniform_(self.hidden_layer.weight, -0.01, 0.01)\n",
        "        nn.init.uniform_(self.hidden_layer.bias, -0.01, 0.01)\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(hidden_dim, tagset_size)\n",
        "        nn.init.uniform_(self.output_layer.weight, -0.01, 0.01)\n",
        "        nn.init.uniform_(self.output_layer.bias, -0.01, 0.01)\n",
        "\n",
        "    def initialize_pretrained_weights(self, vocab_size, embedding_dim, pretrained_embeddings, word_to_ix):\n",
        "        pretrained_weight = np.random.uniform(-0.01, 0.01, (vocab_size, embedding_dim))\n",
        "        for word, idx in word_to_ix.items():\n",
        "            if word in pretrained_embeddings:\n",
        "                pretrained_weight[idx] = pretrained_embeddings[word]\n",
        "        return pretrained_weight\n",
        "\n",
        "    def forward(self, inputs, features):\n",
        "        # Get the embeddings for the input word indices\n",
        "        embeds = self.embeddings(inputs).view(-1)\n",
        "\n",
        "        # Ensure that features are not updated during backpropagation\n",
        "        with torch.no_grad():\n",
        "            features = features.squeeze(0)\n",
        "\n",
        "        # Concatenate the features with the embeddings\n",
        "        combined_input = torch.cat((embeds, features), dim=0)\n",
        "\n",
        "        # Pass through the hidden layer\n",
        "        hidden_out = torch.tanh(self.hidden_layer(combined_input))\n",
        "\n",
        "        # Output layer\n",
        "        tag_scores = self.output_layer(hidden_out)\n",
        "\n",
        "        return tag_scores\n",
        "\n",
        "\n",
        "def train_model_features_with_embeddings(train_data_path, dev_data_path, test_data_path, embedding_path, context_window):\n",
        "    # Hyperparameters\n",
        "    EMBEDDING_DIM = 50\n",
        "    HIDDEN_DIM = 128\n",
        "    FEATURE_DIM = 5\n",
        "    LEARNING_RATE = 0.02\n",
        "    EPOCHS = 10\n",
        "\n",
        "    # Load data\n",
        "    train_sentences = load_data(train_data_path)\n",
        "    dev_sentences = load_data(dev_data_path)\n",
        "    test_sentences = load_data(test_data_path)\n",
        "\n",
        "    # Load pretrained embeddings\n",
        "    pretrained_embeddings = load_pretrained_embeddings(embedding_path, EMBEDDING_DIM)\n",
        "\n",
        "    # Create vocabulary and tag set mappings\n",
        "    word_to_ix = defaultdict(lambda: len(word_to_ix))\n",
        "    tag_to_ix = defaultdict(lambda: len(tag_to_ix))\n",
        "\n",
        "    # Add special tokens to the vocabulary\n",
        "    word_to_ix[START_TOKEN]\n",
        "    word_to_ix[END_TOKEN]\n",
        "    word_to_ix[UNKNOWN_TOKEN]\n",
        "\n",
        "    # Build vocab and tag index from training data only\n",
        "    for sentence in train_sentences:\n",
        "        for word, tag in sentence:\n",
        "            word_to_ix[word]\n",
        "            tag_to_ix[tag]\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = POSTaggingDataset5(train_sentences, word_to_ix, tag_to_ix, pretrained_embeddings, EMBEDDING_DIM, context_window)\n",
        "    dev_dataset = POSTaggingDataset5(dev_sentences, word_to_ix, tag_to_ix, pretrained_embeddings, EMBEDDING_DIM, context_window)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    # Initialize model and optimizer\n",
        "    model = FFNNTagger5(len(word_to_ix), len(tag_to_ix), EMBEDDING_DIM, HIDDEN_DIM, context_window, FEATURE_DIM, pretrained_embeddings, word_to_ix)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    best_dev_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for context_indices, features, target_index in train_loader:\n",
        "            model.zero_grad()\n",
        "\n",
        "            log_probs = model(context_indices, features)\n",
        "\n",
        "            loss = loss_function(log_probs.view(1, -1), target_index)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "        # Evaluate on DEV set after each epoch\n",
        "        dev_accuracy = evaluate_model_features_with_embeddings(model, dev_dataset)\n",
        "        print(f\"DEV Accuracy: {dev_accuracy:.2f}%\")\n",
        "\n",
        "        if dev_accuracy > best_dev_accuracy:\n",
        "            best_dev_accuracy = dev_accuracy\n",
        "\n",
        "    # Final evaluation on DEVTEST set\n",
        "    test_dataset = POSTaggingDataset5(test_sentences, word_to_ix, tag_to_ix, pretrained_embeddings, EMBEDDING_DIM, context_window)\n",
        "    test_accuracy = evaluate_model_features_with_embeddings(model, test_dataset)\n",
        "    print(f\"Final DEVTEST Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "def evaluate_model_features_with_embeddings(model, dataset):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for context_indices, features, target_index in DataLoader(dataset):\n",
        "            log_probs = model(context_indices, features)\n",
        "\n",
        "            predicted_index = torch.argmax(log_probs).item()\n",
        "            correct_predictions += (predicted_index == target_index.item())\n",
        "\n",
        "    accuracy_percentage = (correct_predictions / len(dataset)) * 100\n",
        "    return accuracy_percentage\n",
        "\n",
        "print(\"w = 0\")\n",
        "train_model_features_with_embeddings('twpos-train.tsv',\n",
        "                                     'twpos-dev.tsv',\n",
        "                                     'twpos-devtest.tsv',\n",
        "                                     'twitter-embeddings.txt',\n",
        "                                     context_window=0)  # For w=0 (no context window)\n",
        "print()\n",
        "print(\"w = 1\")\n",
        "train_model_features_with_embeddings('twpos-train.tsv',\n",
        "                                     'twpos-dev.tsv',\n",
        "                                     'twpos-devtest.tsv',\n",
        "                                     'twitter-embeddings.txt',\n",
        "                                     context_window=1)  # For w=1 (context window of size 1)"
      ],
      "metadata": {
        "id": "_XPMf9xsxT6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b857e3e-587a-4548-8755-4c4acb9c7179"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = 0\n",
            "Epoch 1/10, Loss: 17184.7456\n",
            "DEV Accuracy: 76.62%\n",
            "Epoch 2/10, Loss: 7406.7630\n",
            "DEV Accuracy: 76.69%\n",
            "Epoch 3/10, Loss: 5572.6315\n",
            "DEV Accuracy: 77.18%\n",
            "Epoch 4/10, Loss: 4874.6868\n",
            "DEV Accuracy: 76.98%\n",
            "Epoch 5/10, Loss: 4432.6920\n",
            "DEV Accuracy: 77.31%\n",
            "Epoch 6/10, Loss: 4177.1593\n",
            "DEV Accuracy: 76.81%\n",
            "Epoch 7/10, Loss: 4056.4210\n",
            "DEV Accuracy: 76.56%\n",
            "Epoch 8/10, Loss: 3940.1602\n",
            "DEV Accuracy: 77.00%\n",
            "Epoch 9/10, Loss: 3781.5037\n",
            "DEV Accuracy: 77.85%\n",
            "Epoch 10/10, Loss: 3708.0340\n",
            "DEV Accuracy: 76.71%\n",
            "Final DEVTEST Accuracy: 77.54%\n",
            "\n",
            "w = 1\n",
            "Epoch 1/10, Loss: 15192.2301\n",
            "DEV Accuracy: 80.83%\n",
            "Epoch 2/10, Loss: 5682.0385\n",
            "DEV Accuracy: 80.61%\n",
            "Epoch 3/10, Loss: 3459.4999\n",
            "DEV Accuracy: 82.12%\n",
            "Epoch 4/10, Loss: 2258.5599\n",
            "DEV Accuracy: 81.50%\n",
            "Epoch 5/10, Loss: 1680.1238\n",
            "DEV Accuracy: 82.02%\n",
            "Epoch 6/10, Loss: 1234.2076\n",
            "DEV Accuracy: 81.85%\n",
            "Epoch 7/10, Loss: 921.9547\n",
            "DEV Accuracy: 82.00%\n",
            "Epoch 8/10, Loss: 676.8625\n",
            "DEV Accuracy: 82.56%\n",
            "Epoch 9/10, Loss: 536.8922\n",
            "DEV Accuracy: 82.24%\n",
            "Epoch 10/10, Loss: 435.1802\n",
            "DEV Accuracy: 82.20%\n",
            "Final DEVTEST Accuracy: 83.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4.1 Architecture Engineering"
      ],
      "metadata": {
        "id": "hRLKI-Ng5MuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FFNNTaggerLayers(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim1, hidden_dim2, num_hidden_layers, context_window, feature_dim, pretrained_embeddings, word_to_ix):\n",
        "        super(FFNNTaggerLayers, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.context_window_size = 1 + 2 * context_window\n",
        "        self.feature_dim = feature_dim\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "\n",
        "        # Initialize an embedding layer using pretrained embeddings\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Initialize the embedding weights with the pretrained embeddings\n",
        "        pretrained_weight = self.initialize_pretrained_weights(vocab_size, embedding_dim, pretrained_embeddings, word_to_ix)\n",
        "        self.embeddings.weight.data.copy_(torch.tensor(pretrained_weight))\n",
        "\n",
        "        # Calculate input size to the first hidden layer\n",
        "        input_size = self.context_window_size * embedding_dim + feature_dim\n",
        "\n",
        "        # Define hidden layers based on number of layers\n",
        "        if num_hidden_layers == 0:\n",
        "            # No hidden layer, directly connect to the output layer\n",
        "            self.output_layer = nn.Linear(input_size, tagset_size)\n",
        "        elif num_hidden_layers == 1:\n",
        "            self.hidden_layer1 = nn.Linear(input_size, hidden_dim1)\n",
        "            self.output_layer = nn.Linear(hidden_dim1, tagset_size)\n",
        "        elif num_hidden_layers == 2:\n",
        "            self.hidden_layer1 = nn.Linear(input_size, hidden_dim1)\n",
        "            self.hidden_layer2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "            self.output_layer = nn.Linear(hidden_dim2, tagset_size)\n",
        "\n",
        "        # Initialize weights for hidden layers and output layer\n",
        "        nn.init.uniform_(self.output_layer.weight, -0.01, 0.01)\n",
        "        nn.init.uniform_(self.output_layer.bias, -0.01, 0.01)\n",
        "\n",
        "        if num_hidden_layers >= 1:\n",
        "            nn.init.uniform_(self.hidden_layer1.weight, -0.01, 0.01)\n",
        "            nn.init.uniform_(self.hidden_layer1.bias, -0.01, 0.01)\n",
        "\n",
        "        if num_hidden_layers == 2:\n",
        "            nn.init.uniform_(self.hidden_layer2.weight, -0.01, 0.01)\n",
        "            nn.init.uniform_(self.hidden_layer2.bias, -0.01, 0.01)\n",
        "\n",
        "    def initialize_pretrained_weights(self, vocab_size, embedding_dim, pretrained_embeddings, word_to_ix):\n",
        "        pretrained_weight = np.random.uniform(-0.01, 0.01, (vocab_size, embedding_dim))\n",
        "        for word, idx in word_to_ix.items():\n",
        "            if word in pretrained_embeddings:\n",
        "                pretrained_weight[idx] = pretrained_embeddings[word]\n",
        "        return pretrained_weight\n",
        "\n",
        "    def forward(self, inputs, features):\n",
        "        # Get the embeddings for the input word indices\n",
        "        embeds = self.embeddings(inputs).view(-1)  # Flatten the embeddings\n",
        "\n",
        "        # Ensure that features are not updated during backpropagation\n",
        "        with torch.no_grad():\n",
        "            features = features.squeeze(0)  # Squeeze features to remove batch dimension\n",
        "\n",
        "        # Concatenate the features with the embeddings\n",
        "        combined_input = torch.cat((embeds, features), dim=0)\n",
        "\n",
        "        # Pass through the hidden layers based on the number of layers\n",
        "        if self.num_hidden_layers == 0:\n",
        "            hidden_out = combined_input  # No hidden layer\n",
        "        elif self.num_hidden_layers == 1:\n",
        "            hidden_out = torch.tanh(self.hidden_layer1(combined_input))\n",
        "        elif self.num_hidden_layers == 2:\n",
        "            hidden_out = torch.tanh(self.hidden_layer1(combined_input))\n",
        "            hidden_out = torch.tanh(self.hidden_layer2(hidden_out))\n",
        "\n",
        "        # Output layer\n",
        "        tag_scores = self.output_layer(hidden_out)\n",
        "\n",
        "        return tag_scores\n",
        "\n",
        "def train_model_with_layers(train_data_path, dev_data_path, test_data_path, embedding_path, context_window, num_hidden_layers, hidden_dim1, hidden_dim2):\n",
        "    # Hyperparameters\n",
        "    EMBEDDING_DIM = 50\n",
        "    FEATURE_DIM = 5\n",
        "    LEARNING_RATE = 0.02\n",
        "    EPOCHS = 10\n",
        "\n",
        "    # Load data\n",
        "    train_sentences = load_data(train_data_path)\n",
        "    dev_sentences = load_data(dev_data_path)\n",
        "    test_sentences = load_data(test_data_path)\n",
        "\n",
        "    # Load pretrained embeddings\n",
        "    pretrained_embeddings = load_pretrained_embeddings(embedding_path, EMBEDDING_DIM)\n",
        "\n",
        "    # Create vocabulary and tag set mappings\n",
        "    word_to_ix = defaultdict(lambda: len(word_to_ix))\n",
        "    tag_to_ix = defaultdict(lambda: len(tag_to_ix))\n",
        "\n",
        "    # Add special tokens to the vocabulary\n",
        "    word_to_ix[START_TOKEN]\n",
        "    word_to_ix[END_TOKEN]\n",
        "    word_to_ix[UNKNOWN_TOKEN]\n",
        "\n",
        "    # Build vocab and tag index from training data only\n",
        "    for sentence in train_sentences:\n",
        "        for word, tag in sentence:\n",
        "            word_to_ix[word]\n",
        "            tag_to_ix[tag]\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = POSTaggingDataset5(train_sentences, word_to_ix, tag_to_ix, pretrained_embeddings, EMBEDDING_DIM, context_window)\n",
        "    dev_dataset = POSTaggingDataset5(dev_sentences, word_to_ix, tag_to_ix, pretrained_embeddings, EMBEDDING_DIM, context_window)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    # Initialize model and optimizer\n",
        "    model = FFNNTaggerLayers(len(word_to_ix), len(tag_to_ix), EMBEDDING_DIM, hidden_dim1, hidden_dim2, num_hidden_layers, context_window, FEATURE_DIM, pretrained_embeddings, word_to_ix)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    best_dev_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for context_indices, features, target_index in train_loader:\n",
        "            model.zero_grad()\n",
        "\n",
        "            log_probs = model(context_indices, features)\n",
        "\n",
        "            loss = loss_function(log_probs.view(1, -1), target_index)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "        # Evaluate on DEV set after each epoch\n",
        "        dev_accuracy = evaluate_model_with_layers(model, dev_dataset)\n",
        "        print(f\"DEV Accuracy: {dev_accuracy:.2f}%\")\n",
        "\n",
        "        if dev_accuracy > best_dev_accuracy:\n",
        "            best_dev_accuracy = dev_accuracy\n",
        "\n",
        "    # Final evaluation on DEVTEST set\n",
        "    test_dataset = POSTaggingDataset5(test_sentences, word_to_ix, tag_to_ix, pretrained_embeddings, EMBEDDING_DIM, context_window)\n",
        "    test_accuracy = evaluate_model_with_layers(model, test_dataset)\n",
        "    print(f\"Final DEVTEST Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "def evaluate_model_with_layers(model, dataset):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for context_indices, features, target_index in DataLoader(dataset):\n",
        "            log_probs = model(context_indices, features)\n",
        "\n",
        "            predicted_index = torch.argmax(log_probs).item()\n",
        "            correct_predictions += (predicted_index == target_index.item())\n",
        "\n",
        "    accuracy_percentage = (correct_predictions / len(dataset)) * 100\n",
        "    return accuracy_percentage\n",
        "\n",
        "# 0 Hidden Layers\n",
        "print(\"Running with 0 hidden layers\")\n",
        "train_model_with_layers('twpos-train.tsv', 'twpos-dev.tsv', 'twpos-devtest.tsv', 'twitter-embeddings.txt', context_window=1, num_hidden_layers=0, hidden_dim1=0, hidden_dim2=0)\n",
        "\n",
        "# 1 Hidden Layer, size 256\n",
        "print(\"\\nRunning with 1 hidden layer, size 256\")\n",
        "train_model_with_layers('twpos-train.tsv', 'twpos-dev.tsv', 'twpos-devtest.tsv', 'twitter-embeddings.txt', context_window=1, num_hidden_layers=1, hidden_dim1=256, hidden_dim2=0)\n",
        "\n",
        "# 1 Hidden Layer, size 512\n",
        "print(\"\\nRunning with 1 hidden layer, size 512\")\n",
        "train_model_with_layers('twpos-train.tsv', 'twpos-dev.tsv', 'twpos-devtest.tsv', 'twitter-embeddings.txt', context_window=1, num_hidden_layers=1, hidden_dim1=512, hidden_dim2=0)\n",
        "\n",
        "# 2 Hidden Layers, first layer 256, second layer 256\n",
        "print(\"\\nRunning with 2 hidden layers, first layer 256, second layer 256\")\n",
        "train_model_with_layers('twpos-train.tsv', 'twpos-dev.tsv', 'twpos-devtest.tsv', 'twitter-embeddings.txt', context_window=1, num_hidden_layers=2, hidden_dim1=256, hidden_dim2=256)\n",
        "\n",
        "# 2 Hidden Layers, first layer 256, second layer 512\n",
        "print(\"\\nRunning with 2 hidden layers, first layer 256, second layer 512\")\n",
        "train_model_with_layers('twpos-train.tsv', 'twpos-dev.tsv', 'twpos-devtest.tsv', 'twitter-embeddings.txt', context_window=1, num_hidden_layers=2, hidden_dim1=256, hidden_dim2=512)\n",
        "\n",
        "# 2 Hidden Layers, first layer 512, second layer 256\n",
        "print(\"\\nRunning with 2 hidden layers, first layer 512, second layer 256\")\n",
        "train_model_with_layers('twpos-train.tsv', 'twpos-dev.tsv', 'twpos-devtest.tsv', 'twitter-embeddings.txt', context_window=1, num_hidden_layers=2, hidden_dim1=512, hidden_dim2=256)\n",
        "\n",
        "# 2 Hidden Layers, first layer 512, second layer 512\n",
        "print(\"\\nRunning with 2 hidden layers, first layer 512, second layer 512\")\n",
        "train_model_with_layers('twpos-train.tsv', 'twpos-dev.tsv', 'twpos-devtest.tsv', 'twitter-embeddings.txt', context_window=1, num_hidden_layers=2, hidden_dim1=512, hidden_dim2=512)\n"
      ],
      "metadata": {
        "id": "ki-iaJSi5SmK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3aa92f1c-b9b0-4f4b-ca71-dc21e1e7db5c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running with 0 hidden layers\n",
            "Epoch 1/10, Loss: 14240.5413\n",
            "DEV Accuracy: 81.52%\n",
            "Epoch 2/10, Loss: 6555.8713\n",
            "DEV Accuracy: 82.74%\n",
            "Epoch 3/10, Loss: 4714.7967\n",
            "DEV Accuracy: 82.66%\n",
            "Epoch 4/10, Loss: 3504.5746\n",
            "DEV Accuracy: 82.66%\n",
            "Epoch 5/10, Loss: 2683.3231\n",
            "DEV Accuracy: 82.85%\n",
            "Epoch 6/10, Loss: 2131.2976\n",
            "DEV Accuracy: 82.62%\n",
            "Epoch 7/10, Loss: 1749.2745\n",
            "DEV Accuracy: 82.49%\n",
            "Epoch 8/10, Loss: 1446.7813\n",
            "DEV Accuracy: 82.74%\n",
            "Epoch 9/10, Loss: 1219.1930\n",
            "DEV Accuracy: 82.95%\n",
            "Epoch 10/10, Loss: 1032.9540\n",
            "DEV Accuracy: 82.78%\n",
            "Final DEVTEST Accuracy: 84.09%\n",
            "\n",
            "Running with 1 hidden layer, size 256\n",
            "Epoch 1/10, Loss: 14760.9555\n",
            "DEV Accuracy: 81.58%\n",
            "Epoch 2/10, Loss: 5678.1015\n",
            "DEV Accuracy: 81.35%\n",
            "Epoch 3/10, Loss: 3353.3333\n",
            "DEV Accuracy: 81.83%\n",
            "Epoch 4/10, Loss: 2343.7730\n",
            "DEV Accuracy: 82.49%\n",
            "Epoch 5/10, Loss: 1704.1010\n",
            "DEV Accuracy: 81.12%\n",
            "Epoch 6/10, Loss: 1306.5848\n",
            "DEV Accuracy: 81.93%\n",
            "Epoch 7/10, Loss: 956.4136\n",
            "DEV Accuracy: 81.48%\n",
            "Epoch 8/10, Loss: 756.4993\n",
            "DEV Accuracy: 81.39%\n",
            "Epoch 9/10, Loss: 627.6212\n",
            "DEV Accuracy: 82.33%\n",
            "Epoch 10/10, Loss: 515.9883\n",
            "DEV Accuracy: 82.49%\n",
            "Final DEVTEST Accuracy: 83.12%\n",
            "\n",
            "Running with 1 hidden layer, size 512\n",
            "Epoch 1/10, Loss: 14095.0364\n",
            "DEV Accuracy: 81.21%\n",
            "Epoch 2/10, Loss: 5537.0538\n",
            "DEV Accuracy: 80.67%\n",
            "Epoch 3/10, Loss: 3342.6651\n",
            "DEV Accuracy: 81.29%\n",
            "Epoch 4/10, Loss: 2367.4822\n",
            "DEV Accuracy: 81.21%\n",
            "Epoch 5/10, Loss: 1662.9734\n",
            "DEV Accuracy: 82.10%\n",
            "Epoch 6/10, Loss: 1244.1727\n",
            "DEV Accuracy: 82.18%\n",
            "Epoch 7/10, Loss: 1013.5334\n",
            "DEV Accuracy: 82.64%\n",
            "Epoch 8/10, Loss: 760.0123\n",
            "DEV Accuracy: 81.25%\n",
            "Epoch 9/10, Loss: 597.7262\n",
            "DEV Accuracy: 81.91%\n",
            "Epoch 10/10, Loss: 551.8795\n",
            "DEV Accuracy: 82.35%\n",
            "Final DEVTEST Accuracy: 83.16%\n",
            "\n",
            "Running with 2 hidden layers, first layer 256, second layer 256\n",
            "Epoch 1/10, Loss: 23400.5800\n",
            "DEV Accuracy: 75.46%\n",
            "Epoch 2/10, Loss: 8096.4229\n",
            "DEV Accuracy: 80.27%\n",
            "Epoch 3/10, Loss: 4980.5208\n",
            "DEV Accuracy: 79.13%\n",
            "Epoch 4/10, Loss: 3671.8126\n",
            "DEV Accuracy: 79.92%\n",
            "Epoch 5/10, Loss: 2874.4313\n",
            "DEV Accuracy: 82.31%\n",
            "Epoch 6/10, Loss: 2270.9199\n",
            "DEV Accuracy: 80.94%\n",
            "Epoch 7/10, Loss: 1803.5197\n",
            "DEV Accuracy: 81.71%\n",
            "Epoch 8/10, Loss: 1421.6779\n",
            "DEV Accuracy: 81.00%\n",
            "Epoch 9/10, Loss: 1246.6653\n",
            "DEV Accuracy: 81.62%\n",
            "Epoch 10/10, Loss: 1001.5998\n",
            "DEV Accuracy: 81.00%\n",
            "Final DEVTEST Accuracy: 82.50%\n",
            "\n",
            "Running with 2 hidden layers, first layer 256, second layer 512\n",
            "Epoch 1/10, Loss: 21746.8245\n",
            "DEV Accuracy: 77.08%\n",
            "Epoch 2/10, Loss: 7730.5849\n",
            "DEV Accuracy: 79.38%\n",
            "Epoch 3/10, Loss: 5060.4434\n",
            "DEV Accuracy: 78.45%\n",
            "Epoch 4/10, Loss: 3771.1906\n",
            "DEV Accuracy: 81.06%\n",
            "Epoch 5/10, Loss: 2961.6327\n",
            "DEV Accuracy: 80.92%\n",
            "Epoch 6/10, Loss: 2350.1637\n",
            "DEV Accuracy: 80.69%\n",
            "Epoch 7/10, Loss: 1812.5778\n",
            "DEV Accuracy: 80.50%\n",
            "Epoch 8/10, Loss: 1507.4142\n",
            "DEV Accuracy: 80.25%\n",
            "Epoch 9/10, Loss: 1355.6715\n",
            "DEV Accuracy: 81.48%\n",
            "Epoch 10/10, Loss: 1181.3733\n",
            "DEV Accuracy: 79.26%\n",
            "Final DEVTEST Accuracy: 81.29%\n",
            "\n",
            "Running with 2 hidden layers, first layer 512, second layer 256\n",
            "Epoch 1/10, Loss: 21694.4592\n",
            "DEV Accuracy: 78.55%\n",
            "Epoch 2/10, Loss: 7595.8538\n",
            "DEV Accuracy: 78.30%\n",
            "Epoch 3/10, Loss: 4855.7242\n",
            "DEV Accuracy: 80.75%\n",
            "Epoch 4/10, Loss: 3587.2003\n",
            "DEV Accuracy: 79.84%\n",
            "Epoch 5/10, Loss: 2836.3643\n",
            "DEV Accuracy: 80.58%\n",
            "Epoch 6/10, Loss: 2244.3013\n",
            "DEV Accuracy: 80.88%\n",
            "Epoch 7/10, Loss: 1798.3246\n",
            "DEV Accuracy: 81.79%\n",
            "Epoch 8/10, Loss: 1490.0069\n",
            "DEV Accuracy: 81.37%\n",
            "Epoch 9/10, Loss: 1254.0259\n",
            "DEV Accuracy: 80.69%\n",
            "Epoch 10/10, Loss: 1110.7845\n",
            "DEV Accuracy: 79.65%\n",
            "Final DEVTEST Accuracy: 81.20%\n",
            "\n",
            "Running with 2 hidden layers, first layer 512, second layer 512\n",
            "Epoch 1/10, Loss: 20343.1867\n",
            "DEV Accuracy: 79.11%\n",
            "Epoch 2/10, Loss: 7316.3094\n",
            "DEV Accuracy: 80.46%\n",
            "Epoch 3/10, Loss: 4756.0571\n",
            "DEV Accuracy: 79.71%\n",
            "Epoch 4/10, Loss: 3548.0205\n",
            "DEV Accuracy: 80.02%\n",
            "Epoch 5/10, Loss: 2864.3853\n",
            "DEV Accuracy: 81.56%\n",
            "Epoch 6/10, Loss: 2270.6729\n",
            "DEV Accuracy: 80.44%\n",
            "Epoch 7/10, Loss: 1980.0081\n",
            "DEV Accuracy: 80.50%\n",
            "Epoch 8/10, Loss: 1480.9934\n",
            "DEV Accuracy: 80.94%\n",
            "Epoch 9/10, Loss: 1494.0110\n",
            "DEV Accuracy: 80.21%\n",
            "Epoch 10/10, Loss: 1311.0651\n",
            "DEV Accuracy: 79.71%\n",
            "Final DEVTEST Accuracy: 81.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4.2 Different Activation Functions\n",
        "\n",
        "Created the model with 1 hidden layer and width of 128. Since this is more simple and performed very well."
      ],
      "metadata": {
        "id": "NHtQcKfoIrzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FFNNTaggerActivation(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim, context_window, feature_dim, pretrained_embeddings, word_to_ix, activation_function):\n",
        "        super(FFNNTaggerActivation, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.context_window_size = 1 + 2 * context_window\n",
        "        self.feature_dim = feature_dim  # Number of additional features\n",
        "        self.activation_function = activation_function  # Activation function choice\n",
        "\n",
        "        # Initialize an embedding layer using pretrained embeddings\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Initialize the embedding weights with the pretrained embeddings\n",
        "        pretrained_weight = self.initialize_pretrained_weights(vocab_size, embedding_dim, pretrained_embeddings, word_to_ix)\n",
        "        self.embeddings.weight.data.copy_(torch.tensor(pretrained_weight))\n",
        "\n",
        "        # Hidden layer: Adjust input to also include the extra feature dimensions\n",
        "        input_size = self.context_window_size * embedding_dim + feature_dim\n",
        "        self.hidden_layer = nn.Linear(input_size, hidden_dim)\n",
        "        nn.init.uniform_(self.hidden_layer.weight, -0.01, 0.01)\n",
        "        nn.init.uniform_(self.hidden_layer.bias, -0.01, 0.01)\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(hidden_dim, tagset_size)\n",
        "        nn.init.uniform_(self.output_layer.weight, -0.01, 0.01)\n",
        "        nn.init.uniform_(self.output_layer.bias, -0.01, 0.01)\n",
        "\n",
        "    def initialize_pretrained_weights(self, vocab_size, embedding_dim, pretrained_embeddings, word_to_ix):\n",
        "        pretrained_weight = np.random.uniform(-0.01, 0.01, (vocab_size, embedding_dim))\n",
        "        for word, idx in word_to_ix.items():\n",
        "            if word in pretrained_embeddings:\n",
        "                pretrained_weight[idx] = pretrained_embeddings[word]\n",
        "        return pretrained_weight\n",
        "\n",
        "    def forward(self, inputs, features):\n",
        "        # Get the embeddings for the input word indices\n",
        "        embeds = self.embeddings(inputs).view(-1)  # Flatten the embeddings\n",
        "\n",
        "        # Ensure that features are not updated during backpropagation\n",
        "        with torch.no_grad():\n",
        "            features = features.squeeze(0)  # Squeeze features to remove batch dimension\n",
        "\n",
        "        # Concatenate the features with the embeddings\n",
        "        combined_input = torch.cat((embeds, features), dim=0)\n",
        "\n",
        "        # Apply the hidden layer\n",
        "        hidden_out = self.hidden_layer(combined_input)\n",
        "\n",
        "        # Apply the selected activation function\n",
        "        if self.activation_function == 'identity':\n",
        "            activated_out = hidden_out  # Identity activation (no transformation)\n",
        "        elif self.activation_function == 'relu':\n",
        "            activated_out = torch.relu(hidden_out)  # ReLU activation\n",
        "        elif self.activation_function == 'sigmoid':\n",
        "            activated_out = torch.sigmoid(hidden_out)  # Sigmoid activation\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported activation function\")\n",
        "\n",
        "        # Output layer\n",
        "        tag_scores = self.output_layer(activated_out)\n",
        "\n",
        "        return tag_scores\n",
        "\n",
        "def train_model_with_activation(train_data_path, dev_data_path, test_data_path, embedding_path, context_window, activation_function):\n",
        "    # Hyperparameters\n",
        "    EMBEDDING_DIM = 50\n",
        "    HIDDEN_DIM = 128  # Single hidden layer with size 128\n",
        "    FEATURE_DIM = 5\n",
        "    LEARNING_RATE = 0.02\n",
        "    EPOCHS = 10\n",
        "\n",
        "    # Load data\n",
        "    train_sentences = load_data(train_data_path)\n",
        "    dev_sentences = load_data(dev_data_path)\n",
        "    test_sentences = load_data(test_data_path)\n",
        "\n",
        "    # Load pretrained embeddings\n",
        "    pretrained_embeddings = load_pretrained_embeddings(embedding_path, EMBEDDING_DIM)\n",
        "\n",
        "    # Create vocabulary and tag set mappings\n",
        "    word_to_ix = defaultdict(lambda: len(word_to_ix))\n",
        "    tag_to_ix = defaultdict(lambda: len(tag_to_ix))\n",
        "\n",
        "    # Add special tokens to the vocabulary\n",
        "    word_to_ix[START_TOKEN]\n",
        "    word_to_ix[END_TOKEN]\n",
        "    word_to_ix[UNKNOWN_TOKEN]\n",
        "\n",
        "    # Build vocab and tag index from training data only\n",
        "    for sentence in train_sentences:\n",
        "        for word, tag in sentence:\n",
        "            word_to_ix[word]\n",
        "            tag_to_ix[tag]\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = POSTaggingDataset5(train_sentences, word_to_ix, tag_to_ix, pretrained_embeddings, EMBEDDING_DIM, context_window)\n",
        "    dev_dataset = POSTaggingDataset5(dev_sentences, word_to_ix, tag_to_ix, pretrained_embeddings, EMBEDDING_DIM, context_window)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    # Initialize model and optimizer\n",
        "    model = FFNNTaggerActivation(len(word_to_ix), len(tag_to_ix), EMBEDDING_DIM, HIDDEN_DIM, context_window, FEATURE_DIM, pretrained_embeddings, word_to_ix, activation_function)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    best_dev_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for context_indices, features, target_index in train_loader:\n",
        "            model.zero_grad()\n",
        "\n",
        "            log_probs = model(context_indices, features)\n",
        "\n",
        "            loss = loss_function(log_probs.view(1, -1), target_index)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "        # Evaluate on DEV set after each epoch\n",
        "        dev_accuracy = evaluate_model_with_activation(model, dev_dataset)\n",
        "        print(f\"DEV Accuracy: {dev_accuracy:.2f}%\")\n",
        "\n",
        "        if dev_accuracy > best_dev_accuracy:\n",
        "            best_dev_accuracy = dev_accuracy\n",
        "\n",
        "    # Final evaluation on DEVTEST set\n",
        "    test_dataset = POSTaggingDataset5(test_sentences, word_to_ix, tag_to_ix, pretrained_embeddings, EMBEDDING_DIM, context_window)\n",
        "    test_accuracy = evaluate_model_with_activation(model, test_dataset)\n",
        "    print(f\"Final DEVTEST Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "def evaluate_model_with_activation(model, dataset):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for context_indices, features, target_index in DataLoader(dataset):\n",
        "            log_probs = model(context_indices, features)\n",
        "\n",
        "            predicted_index = torch.argmax(log_probs).item()\n",
        "            correct_predictions += (predicted_index == target_index.item())\n",
        "\n",
        "    accuracy_percentage = (correct_predictions / len(dataset)) * 100\n",
        "    return accuracy_percentage\n",
        "\n",
        "# Identity activation\n",
        "print(\"Running with Identity activation\")\n",
        "train_model_with_activation('twpos-train.tsv', 'twpos-dev.tsv', 'twpos-devtest.tsv', 'twitter-embeddings.txt', context_window=1, activation_function='identity')\n",
        "\n",
        "# ReLU activation\n",
        "print(\"\\nRunning with ReLU activation\")\n",
        "train_model_with_activation('twpos-train.tsv', 'twpos-dev.tsv', 'twpos-devtest.tsv', 'twitter-embeddings.txt', context_window=1, activation_function='relu')\n",
        "\n",
        "# Logistic sigmoid activation\n",
        "print(\"\\nRunning with Sigmoid activation\")\n",
        "train_model_with_activation('twpos-train.tsv', 'twpos-dev.tsv', 'twpos-devtest.tsv', 'twitter-embeddings.txt', context_window=1, activation_function='sigmoid')"
      ],
      "metadata": {
        "id": "ozJI8HERACbv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d259155-b548-4c7b-c7bd-a89384490b96"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running with Identity activation\n",
            "Epoch 1/10, Loss: 15179.4199\n",
            "DEV Accuracy: 79.98%\n",
            "Epoch 2/10, Loss: 5749.4629\n",
            "DEV Accuracy: 80.25%\n",
            "Epoch 3/10, Loss: 3566.7782\n",
            "DEV Accuracy: 81.50%\n",
            "Epoch 4/10, Loss: 2477.3792\n",
            "DEV Accuracy: 82.08%\n",
            "Epoch 5/10, Loss: 1901.3412\n",
            "DEV Accuracy: 81.60%\n",
            "Epoch 6/10, Loss: 1351.5979\n",
            "DEV Accuracy: 82.02%\n",
            "Epoch 7/10, Loss: 1098.5174\n",
            "DEV Accuracy: 81.62%\n",
            "Epoch 8/10, Loss: 891.2500\n",
            "DEV Accuracy: 82.20%\n",
            "Epoch 9/10, Loss: 679.6741\n",
            "DEV Accuracy: 81.23%\n",
            "Epoch 10/10, Loss: 614.7116\n",
            "DEV Accuracy: 82.08%\n",
            "Final DEVTEST Accuracy: 83.40%\n",
            "\n",
            "Running with ReLU activation\n",
            "Epoch 1/10, Loss: 15884.5369\n",
            "DEV Accuracy: 79.07%\n",
            "Epoch 2/10, Loss: 5845.6046\n",
            "DEV Accuracy: 81.93%\n",
            "Epoch 3/10, Loss: 3537.6465\n",
            "DEV Accuracy: 81.25%\n",
            "Epoch 4/10, Loss: 2398.7428\n",
            "DEV Accuracy: 81.87%\n",
            "Epoch 5/10, Loss: 1585.8450\n",
            "DEV Accuracy: 82.45%\n",
            "Epoch 6/10, Loss: 1222.4060\n",
            "DEV Accuracy: 81.73%\n",
            "Epoch 7/10, Loss: 779.4733\n",
            "DEV Accuracy: 81.79%\n",
            "Epoch 8/10, Loss: 644.7292\n",
            "DEV Accuracy: 82.33%\n",
            "Epoch 9/10, Loss: 456.2915\n",
            "DEV Accuracy: 81.85%\n",
            "Epoch 10/10, Loss: 312.6364\n",
            "DEV Accuracy: 82.29%\n",
            "Final DEVTEST Accuracy: 83.16%\n",
            "\n",
            "Running with Sigmoid activation\n",
            "Epoch 1/10, Loss: 32369.1949\n",
            "DEV Accuracy: 69.47%\n",
            "Epoch 2/10, Loss: 11341.7422\n",
            "DEV Accuracy: 78.70%\n",
            "Epoch 3/10, Loss: 6856.4804\n",
            "DEV Accuracy: 80.81%\n",
            "Epoch 4/10, Loss: 4626.5106\n",
            "DEV Accuracy: 79.92%\n",
            "Epoch 5/10, Loss: 3408.5703\n",
            "DEV Accuracy: 81.62%\n",
            "Epoch 6/10, Loss: 2705.3852\n",
            "DEV Accuracy: 80.32%\n",
            "Epoch 7/10, Loss: 2255.3660\n",
            "DEV Accuracy: 81.58%\n",
            "Epoch 8/10, Loss: 1830.3409\n",
            "DEV Accuracy: 81.19%\n",
            "Epoch 9/10, Loss: 1502.1520\n",
            "DEV Accuracy: 81.71%\n",
            "Epoch 10/10, Loss: 1274.5842\n",
            "DEV Accuracy: 81.87%\n",
            "Final DEVTEST Accuracy: 82.80%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.4.3 Experiment with w = 2"
      ],
      "metadata": {
        "id": "UtwRgRdaylkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"For context window = 2\")\n",
        "train_model_features_with_embeddings('twpos-train.tsv',\n",
        "                                     'twpos-dev.tsv',\n",
        "                                     'twpos-devtest.tsv',\n",
        "                                     'twitter-embeddings.txt',\n",
        "                                     context_window=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa5lvKMrrbfg",
        "outputId": "cc41de87-d0d5-4876-a04b-1218f05a5e72"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For context window = 2\n",
            "Epoch 1/10, Loss: 15196.5382\n",
            "DEV Accuracy: 79.09%\n",
            "Epoch 2/10, Loss: 5573.0407\n",
            "DEV Accuracy: 81.21%\n",
            "Epoch 3/10, Loss: 3132.5023\n",
            "DEV Accuracy: 80.48%\n",
            "Epoch 4/10, Loss: 1848.4706\n",
            "DEV Accuracy: 81.60%\n",
            "Epoch 5/10, Loss: 1164.6957\n",
            "DEV Accuracy: 82.49%\n",
            "Epoch 6/10, Loss: 699.0670\n",
            "DEV Accuracy: 81.35%\n",
            "Epoch 7/10, Loss: 410.9679\n",
            "DEV Accuracy: 82.10%\n",
            "Epoch 8/10, Loss: 248.2247\n",
            "DEV Accuracy: 82.27%\n",
            "Epoch 9/10, Loss: 150.7192\n",
            "DEV Accuracy: 82.12%\n",
            "Epoch 10/10, Loss: 104.4585\n",
            "DEV Accuracy: 82.08%\n",
            "Final DEVTEST Accuracy: 83.47%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.5 RNN Taggers"
      ],
      "metadata": {
        "id": "YEwKFX8IygTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class POSTaggingDataset6(Dataset):\n",
        "    def __init__(self, sentences, word_to_ix, tag_to_ix, pretrained_embeddings, embedding_dim, context_window=1):\n",
        "        self.sentences = sentences\n",
        "        self.word_to_ix = word_to_ix\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.context_window = context_window\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.pretrained_embeddings = pretrained_embeddings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "\n",
        "        # Context window setup (for padding if needed)\n",
        "        context_indices = []\n",
        "        labels = []\n",
        "\n",
        "        for word, tag in sentence:\n",
        "            # Use <UNK> token if word is not in vocab\n",
        "            word_index = self.word_to_ix.get(word, self.word_to_ix.get(\"<UNK>\", self.word_to_ix.get(\"UUUNKKK\")))\n",
        "            context_indices.append(word_index)\n",
        "            labels.append(self.tag_to_ix[tag])\n",
        "\n",
        "        # Additional feature extraction for each word\n",
        "        features = []\n",
        "        for word, _ in sentence:\n",
        "            # Feature 1: Capitalization (binary)\n",
        "            is_capitalized = 1 if word[0].isupper() else 0\n",
        "            # Feature 2: Common suffixes\n",
        "            has_suffix_ing = 1 if word.endswith(\"ing\") else 0\n",
        "            has_suffix_ed = 1 if word.endswith(\"ed\") else 0\n",
        "            has_suffix_ly = 1 if word.endswith(\"ly\") else 0\n",
        "            # Feature 3: Special characters\n",
        "            contains_special_char = 1 if any(char in \"!@#?&\" for char in word) else 0\n",
        "            features.append([is_capitalized, has_suffix_ing, has_suffix_ed, has_suffix_ly, contains_special_char])\n",
        "\n",
        "        # Convert lists to tensors\n",
        "        context_indices = torch.tensor(context_indices, dtype=torch.long)\n",
        "        labels = torch.tensor(labels, dtype=torch.long)\n",
        "        features = torch.tensor(features, dtype=torch.float32)\n",
        "\n",
        "        return context_indices, features, labels\n",
        "\n",
        "\n",
        "class RNNTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim, num_layers, rnn_type='rnn', bidirectional=False, pretrained_embeddings=None, word_to_ix=None):\n",
        "        super(RNNTagger, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.rnn_type = rnn_type\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        if pretrained_embeddings is not None:\n",
        "            pretrained_weight = self.initialize_pretrained_weights(vocab_size, embedding_dim, pretrained_embeddings, word_to_ix)\n",
        "            self.embeddings.weight.data.copy_(torch.tensor(pretrained_weight))\n",
        "\n",
        "        # Define the RNN (either standard RNN, LSTM, or GRU)\n",
        "        input_dim = embedding_dim + 5\n",
        "        if rnn_type == 'rnn':\n",
        "            self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "        elif rnn_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "        elif rnn_type == 'gru':\n",
        "            self.rnn = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported RNN type: {rnn_type}\")\n",
        "\n",
        "        # Fully connected layer to predict POS tags\n",
        "        if bidirectional:\n",
        "            self.fc = nn.Linear(hidden_dim * 2, tagset_size)\n",
        "        else:\n",
        "            self.fc = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def initialize_pretrained_weights(self, vocab_size, embedding_dim, pretrained_embeddings, word_to_ix):\n",
        "        pretrained_weight = np.random.uniform(-0.01, 0.01, (vocab_size, embedding_dim))\n",
        "        for word, idx in word_to_ix.items():\n",
        "            if word in pretrained_embeddings:\n",
        "                pretrained_weight[idx] = pretrained_embeddings[word]\n",
        "        return pretrained_weight\n",
        "\n",
        "    def forward(self, inputs, features):\n",
        "        # Embedding layer\n",
        "        embeds = self.embeddings(inputs)\n",
        "\n",
        "        # Concatenate features along the embedding dimension\n",
        "        combined_input = torch.cat((embeds, features), dim=2)\n",
        "\n",
        "        # RNN layer (RNN/LSTM/GRU)\n",
        "        if self.rnn_type == 'lstm':\n",
        "            rnn_out, (hidden, cell) = self.rnn(combined_input)\n",
        "        else:\n",
        "            rnn_out, hidden = self.rnn(combined_input)\n",
        "\n",
        "        # Output layer\n",
        "        tag_scores = self.fc(rnn_out)\n",
        "\n",
        "        return tag_scores\n",
        "\n",
        "\n",
        "def train_model_rnn(train_data_path, dev_data_path, test_data_path, embedding_path, rnn_type='rnn', bidirectional=False):\n",
        "    # Hyperparameters\n",
        "    EMBEDDING_DIM = 50\n",
        "    HIDDEN_DIM = 128\n",
        "    FEATURE_DIM = 5  # 5 additional features\n",
        "    LEARNING_RATE = 0.02\n",
        "    EPOCHS = 10\n",
        "    NUM_LAYERS = 1\n",
        "\n",
        "    # Load data\n",
        "    train_sentences = load_data(train_data_path)\n",
        "    dev_sentences = load_data(dev_data_path)\n",
        "    test_sentences = load_data(test_data_path)\n",
        "\n",
        "    # Load pretrained embeddings\n",
        "    pretrained_embeddings = load_pretrained_embeddings(embedding_path, EMBEDDING_DIM)\n",
        "\n",
        "    # Create vocabulary and tag set mappings\n",
        "    word_to_ix = defaultdict(lambda: len(word_to_ix))\n",
        "    tag_to_ix = defaultdict(lambda: len(tag_to_ix))\n",
        "\n",
        "    # Add special tokens to the vocabulary\n",
        "    word_to_ix[\"<PAD>\"]\n",
        "    word_to_ix[\"<UNK>\"]\n",
        "    word_to_ix[\"<START>\"]\n",
        "    word_to_ix[\"<END>\"]\n",
        "\n",
        "    # Build vocab and tag index from training data only\n",
        "    for sentence in train_sentences:\n",
        "        for word, tag in sentence:\n",
        "            word_to_ix[word]\n",
        "            tag_to_ix[tag]\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = POSTaggingDataset6(train_sentences, word_to_ix, tag_to_ix, pretrained_embeddings, EMBEDDING_DIM, context_window=1)\n",
        "    dev_dataset = POSTaggingDataset6(dev_sentences, word_to_ix, tag_to_ix, pretrained_embeddings, EMBEDDING_DIM, context_window=1)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "    # Initialize model and optimizer\n",
        "    model = RNNTagger(len(word_to_ix), len(tag_to_ix), EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, rnn_type=rnn_type, bidirectional=bidirectional, pretrained_embeddings=pretrained_embeddings, word_to_ix=word_to_ix)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training loop\n",
        "    best_dev_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for context_indices, features, target_index in train_loader:\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            log_probs = model(context_indices, features)\n",
        "            log_probs = log_probs.view(-1, len(tag_to_ix))\n",
        "            target_index = target_index.view(-1)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_function(log_probs, target_index)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "        # Evaluate on DEV set after each epoch\n",
        "        dev_accuracy = evaluate_model_rnn(model, dev_dataset)\n",
        "        print(f\"DEV Accuracy: {dev_accuracy:.2f}%\")\n",
        "\n",
        "        if dev_accuracy > best_dev_accuracy:\n",
        "            best_dev_accuracy = dev_accuracy\n",
        "\n",
        "    # Final evaluation on DEVTEST set\n",
        "    test_dataset = POSTaggingDataset6(test_sentences, word_to_ix, tag_to_ix, pretrained_embeddings, EMBEDDING_DIM, context_window=1)\n",
        "    test_accuracy = evaluate_model_rnn(model, test_dataset)\n",
        "    print(f\"Final DEVTEST Accuracy: {test_accuracy:.2f}%\")\n",
        "\n",
        "def evaluate_model_rnn(model, dataset):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0  # Track total predictions to calculate accuracy correctly\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for context_indices, features, target_index in DataLoader(dataset):\n",
        "            log_probs = model(context_indices, features)  # Output logits for each token in sequence\n",
        "            log_probs = log_probs.view(-1, log_probs.size(-1))  # Reshape for comparison\n",
        "            target_index = target_index.view(-1)\n",
        "\n",
        "            # Print shapes to debug\n",
        "            # print(\"log_probs shape:\", log_probs.shape)\n",
        "            # print(\"target_index shape:\", target_index.shape)\n",
        "\n",
        "            # Get predictions\n",
        "            predicted_index = torch.argmax(log_probs, dim=1)\n",
        "\n",
        "            # Compare predictions with true labels\n",
        "            correct_predictions += (predicted_index == target_index).sum().item()\n",
        "            total_predictions += target_index.size(0)\n",
        "\n",
        "    accuracy_percentage = (correct_predictions / total_predictions) * 100\n",
        "    print(f\"Total correct predictions: {correct_predictions}\")\n",
        "    print(f\"Total predictions: {total_predictions}\")\n",
        "    print(f\"Calculated Accuracy: {accuracy_percentage}%\")\n",
        "    return accuracy_percentage\n",
        "\n",
        "\n",
        "# Standard RNN\n",
        "print(\"Running Standard RNN\")\n",
        "train_model_rnn('twpos-train.tsv', 'twpos-dev.tsv', 'twpos-devtest.tsv', 'twitter-embeddings.txt', rnn_type='rnn', bidirectional=False)\n",
        "\n",
        "# Bidirectional RNN\n",
        "print(\"\\nRunning Bidirectional RNN\")\n",
        "train_model_rnn('twpos-train.tsv', 'twpos-dev.tsv', 'twpos-devtest.tsv', 'twitter-embeddings.txt', rnn_type='rnn', bidirectional=True)\n",
        "\n",
        "# LSTM\n",
        "print(\"\\nRunning LSTM\")\n",
        "train_model_rnn('twpos-train.tsv', 'twpos-dev.tsv', 'twpos-devtest.tsv', 'twitter-embeddings.txt', rnn_type='lstm', bidirectional=False)\n",
        "\n",
        "# Bidirectional LSTM\n",
        "print(\"\\nRunning Bidirectional LSTM\")\n",
        "train_model_rnn('twpos-train.tsv', 'twpos-dev.tsv', 'twpos-devtest.tsv', 'twitter-embeddings.txt', rnn_type='lstm', bidirectional=True)\n",
        "\n",
        "# GRU\n",
        "print(\"\\nRunning GRU\")\n",
        "train_model_rnn('twpos-train.tsv', 'twpos-dev.tsv', 'twpos-devtest.tsv', 'twitter-embeddings.txt', rnn_type='gru', bidirectional=False)\n",
        "\n",
        "# Bidirectional GRU\n",
        "print(\"\\nRunning Bidirectional GRU\")\n",
        "train_model_rnn('twpos-train.tsv', 'twpos-dev.tsv', 'twpos-devtest.tsv', 'twitter-embeddings.txt', rnn_type='gru', bidirectional=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfEmOffybbnY",
        "outputId": "f4b504ac-ad6c-433b-d9a1-7672356dae28"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Standard RNN\n",
            "Epoch 1/10, Loss: 2485.8465\n",
            "Total correct predictions: 3009\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 62.41443683883012%\n",
            "DEV Accuracy: 62.41%\n",
            "Epoch 2/10, Loss: 1351.8945\n",
            "Total correct predictions: 3412\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 70.773698402821%\n",
            "DEV Accuracy: 70.77%\n",
            "Epoch 3/10, Loss: 970.3292\n",
            "Total correct predictions: 3625\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 75.1918689068658%\n",
            "DEV Accuracy: 75.19%\n",
            "Epoch 4/10, Loss: 785.5980\n",
            "Total correct predictions: 3773\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 78.26177141671853%\n",
            "DEV Accuracy: 78.26%\n",
            "Epoch 5/10, Loss: 669.7385\n",
            "Total correct predictions: 3826\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 79.36112839659822%\n",
            "DEV Accuracy: 79.36%\n",
            "Epoch 6/10, Loss: 588.5102\n",
            "Total correct predictions: 3893\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 80.75088155984236%\n",
            "DEV Accuracy: 80.75%\n",
            "Epoch 7/10, Loss: 527.3471\n",
            "Total correct predictions: 3910\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 81.1035054967849%\n",
            "DEV Accuracy: 81.10%\n",
            "Epoch 8/10, Loss: 482.9034\n",
            "Total correct predictions: 3916\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 81.22796100394109%\n",
            "DEV Accuracy: 81.23%\n",
            "Epoch 9/10, Loss: 443.5852\n",
            "Total correct predictions: 3927\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 81.45612943372744%\n",
            "DEV Accuracy: 81.46%\n",
            "Epoch 10/10, Loss: 410.4134\n",
            "Total correct predictions: 3938\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 81.68429786351379%\n",
            "DEV Accuracy: 81.68%\n",
            "Total correct predictions: 3846\n",
            "Total predictions: 4639\n",
            "Calculated Accuracy: 82.90579866350507%\n",
            "Final DEVTEST Accuracy: 82.91%\n",
            "\n",
            "Running Bidirectional RNN\n",
            "Epoch 1/10, Loss: 2329.7963\n",
            "Total correct predictions: 3144\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 65.21468574984442%\n",
            "DEV Accuracy: 65.21%\n",
            "Epoch 2/10, Loss: 1174.0817\n",
            "Total correct predictions: 3573\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 74.11325451151214%\n",
            "DEV Accuracy: 74.11%\n",
            "Epoch 3/10, Loss: 841.2749\n",
            "Total correct predictions: 3755\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 77.88840489524995%\n",
            "DEV Accuracy: 77.89%\n",
            "Epoch 4/10, Loss: 683.8069\n",
            "Total correct predictions: 3835\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 79.5478116573325%\n",
            "DEV Accuracy: 79.55%\n",
            "Epoch 5/10, Loss: 582.5557\n",
            "Total correct predictions: 3904\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 80.9790499896287%\n",
            "DEV Accuracy: 80.98%\n",
            "Epoch 6/10, Loss: 511.8363\n",
            "Total correct predictions: 3944\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 81.80875337066999%\n",
            "DEV Accuracy: 81.81%\n",
            "Epoch 7/10, Loss: 459.1787\n",
            "Total correct predictions: 3976\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 82.47251607550301%\n",
            "DEV Accuracy: 82.47%\n",
            "Epoch 8/10, Loss: 417.3217\n",
            "Total correct predictions: 3989\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 82.74216967434143%\n",
            "DEV Accuracy: 82.74%\n",
            "Epoch 9/10, Loss: 382.0577\n",
            "Total correct predictions: 3982\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 82.5969715826592%\n",
            "DEV Accuracy: 82.60%\n",
            "Epoch 10/10, Loss: 351.0521\n",
            "Total correct predictions: 4011\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 83.19850653391413%\n",
            "DEV Accuracy: 83.20%\n",
            "Total correct predictions: 3905\n",
            "Total predictions: 4639\n",
            "Calculated Accuracy: 84.17762448803622%\n",
            "Final DEVTEST Accuracy: 84.18%\n",
            "\n",
            "Running LSTM\n",
            "Epoch 1/10, Loss: 3310.3774\n",
            "Total correct predictions: 1221\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 25.326695706285%\n",
            "DEV Accuracy: 25.33%\n",
            "Epoch 2/10, Loss: 2935.7777\n",
            "Total correct predictions: 1662\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 34.47417548226509%\n",
            "DEV Accuracy: 34.47%\n",
            "Epoch 3/10, Loss: 2592.5189\n",
            "Total correct predictions: 1865\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 38.684920141049574%\n",
            "DEV Accuracy: 38.68%\n",
            "Epoch 4/10, Loss: 2155.0326\n",
            "Total correct predictions: 2603\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 53.99294752126115%\n",
            "DEV Accuracy: 53.99%\n",
            "Epoch 5/10, Loss: 1703.9760\n",
            "Total correct predictions: 3046\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 63.1819124662933%\n",
            "DEV Accuracy: 63.18%\n",
            "Epoch 6/10, Loss: 1346.1718\n",
            "Total correct predictions: 3326\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 68.98983613358224%\n",
            "DEV Accuracy: 68.99%\n",
            "Epoch 7/10, Loss: 1115.7096\n",
            "Total correct predictions: 3461\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 71.79008504459655%\n",
            "DEV Accuracy: 71.79%\n",
            "Epoch 8/10, Loss: 960.3303\n",
            "Total correct predictions: 3579\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 74.23771001866832%\n",
            "DEV Accuracy: 74.24%\n",
            "Epoch 9/10, Loss: 847.3546\n",
            "Total correct predictions: 3717\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 77.10018668326073%\n",
            "DEV Accuracy: 77.10%\n",
            "Epoch 10/10, Loss: 756.5697\n",
            "Total correct predictions: 3726\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 77.28686994399501%\n",
            "DEV Accuracy: 77.29%\n",
            "Total correct predictions: 3643\n",
            "Total predictions: 4639\n",
            "Calculated Accuracy: 78.52985557232162%\n",
            "Final DEVTEST Accuracy: 78.53%\n",
            "\n",
            "Running Bidirectional LSTM\n",
            "Epoch 1/10, Loss: 3267.6943\n",
            "Total correct predictions: 1330\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 27.587637419622485%\n",
            "DEV Accuracy: 27.59%\n",
            "Epoch 2/10, Loss: 2813.4838\n",
            "Total correct predictions: 1584\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 32.8562538892346%\n",
            "DEV Accuracy: 32.86%\n",
            "Epoch 3/10, Loss: 2456.8665\n",
            "Total correct predictions: 1910\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 39.61833644472102%\n",
            "DEV Accuracy: 39.62%\n",
            "Epoch 4/10, Loss: 2073.9639\n",
            "Total correct predictions: 2711\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 56.2331466500726%\n",
            "DEV Accuracy: 56.23%\n",
            "Epoch 5/10, Loss: 1694.3261\n",
            "Total correct predictions: 3062\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 63.51379381870981%\n",
            "DEV Accuracy: 63.51%\n",
            "Epoch 6/10, Loss: 1376.8960\n",
            "Total correct predictions: 3268\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 67.78676623107239%\n",
            "DEV Accuracy: 67.79%\n",
            "Epoch 7/10, Loss: 1146.0323\n",
            "Total correct predictions: 3454\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 71.64488695291433%\n",
            "DEV Accuracy: 71.64%\n",
            "Epoch 8/10, Loss: 982.7346\n",
            "Total correct predictions: 3575\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 74.15473968056419%\n",
            "DEV Accuracy: 74.15%\n",
            "Epoch 9/10, Loss: 862.9236\n",
            "Total correct predictions: 3664\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 76.00082970338104%\n",
            "DEV Accuracy: 76.00%\n",
            "Epoch 10/10, Loss: 769.5733\n",
            "Total correct predictions: 3793\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 78.67662310723917%\n",
            "DEV Accuracy: 78.68%\n",
            "Total correct predictions: 3701\n",
            "Total predictions: 4639\n",
            "Calculated Accuracy: 79.78012502694546%\n",
            "Final DEVTEST Accuracy: 79.78%\n",
            "\n",
            "Running GRU\n",
            "Epoch 1/10, Loss: 3051.9207\n",
            "Total correct predictions: 1805\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 37.440365069487655%\n",
            "DEV Accuracy: 37.44%\n",
            "Epoch 2/10, Loss: 2275.3844\n",
            "Total correct predictions: 2739\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 56.8139390168015%\n",
            "DEV Accuracy: 56.81%\n",
            "Epoch 3/10, Loss: 1561.9131\n",
            "Total correct predictions: 3250\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 67.41339970960382%\n",
            "DEV Accuracy: 67.41%\n",
            "Epoch 4/10, Loss: 1144.0668\n",
            "Total correct predictions: 3487\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 72.32939224227339%\n",
            "DEV Accuracy: 72.33%\n",
            "Epoch 5/10, Loss: 927.3157\n",
            "Total correct predictions: 3666\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 76.04231487243311%\n",
            "DEV Accuracy: 76.04%\n",
            "Epoch 6/10, Loss: 791.2430\n",
            "Total correct predictions: 3767\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 78.13731590956233%\n",
            "DEV Accuracy: 78.14%\n",
            "Epoch 7/10, Loss: 695.0243\n",
            "Total correct predictions: 3802\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 78.86330636797345%\n",
            "DEV Accuracy: 78.86%\n",
            "Epoch 8/10, Loss: 622.3462\n",
            "Total correct predictions: 3880\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 80.48122796100394%\n",
            "DEV Accuracy: 80.48%\n",
            "Epoch 9/10, Loss: 564.9579\n",
            "Total correct predictions: 3916\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 81.22796100394109%\n",
            "DEV Accuracy: 81.23%\n",
            "Epoch 10/10, Loss: 518.4090\n",
            "Total correct predictions: 3925\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 81.41464426467537%\n",
            "DEV Accuracy: 81.41%\n",
            "Total correct predictions: 3840\n",
            "Total predictions: 4639\n",
            "Calculated Accuracy: 82.77646044406121%\n",
            "Final DEVTEST Accuracy: 82.78%\n",
            "\n",
            "Running Bidirectional GRU\n",
            "Epoch 1/10, Loss: 2922.5646\n",
            "Total correct predictions: 1862\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 38.62269238747148%\n",
            "DEV Accuracy: 38.62%\n",
            "Epoch 2/10, Loss: 2087.5821\n",
            "Total correct predictions: 2887\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 59.88384152665422%\n",
            "DEV Accuracy: 59.88%\n",
            "Epoch 3/10, Loss: 1466.5262\n",
            "Total correct predictions: 3328\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 69.03132130263431%\n",
            "DEV Accuracy: 69.03%\n",
            "Epoch 4/10, Loss: 1095.7349\n",
            "Total correct predictions: 3530\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 73.22132337689277%\n",
            "DEV Accuracy: 73.22%\n",
            "Epoch 5/10, Loss: 893.1863\n",
            "Total correct predictions: 3693\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 76.60236465463596%\n",
            "DEV Accuracy: 76.60%\n",
            "Epoch 6/10, Loss: 765.1481\n",
            "Total correct predictions: 3800\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 78.82182119892138%\n",
            "DEV Accuracy: 78.82%\n",
            "Epoch 7/10, Loss: 671.6129\n",
            "Total correct predictions: 3864\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 80.14934660858744%\n",
            "DEV Accuracy: 80.15%\n",
            "Epoch 8/10, Loss: 600.1929\n",
            "Total correct predictions: 3890\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 80.68865380626427%\n",
            "DEV Accuracy: 80.69%\n",
            "Epoch 9/10, Loss: 544.0358\n",
            "Total correct predictions: 3870\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 80.27380211574362%\n",
            "DEV Accuracy: 80.27%\n",
            "Epoch 10/10, Loss: 499.8363\n",
            "Total correct predictions: 3957\n",
            "Total predictions: 4821\n",
            "Calculated Accuracy: 82.07840696950839%\n",
            "DEV Accuracy: 82.08%\n",
            "Total correct predictions: 3845\n",
            "Total predictions: 4639\n",
            "Calculated Accuracy: 82.88424229359775%\n",
            "Final DEVTEST Accuracy: 82.88%\n"
          ]
        }
      ]
    }
  ]
}